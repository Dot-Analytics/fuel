{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "r44t72cguuux3a6m6lhm",
   "authorId": "8227038915097",
   "authorName": "DAVIDSANDERS",
   "authorEmail": "david.sanders@dotfoods.com",
   "sessionId": "bade6ffd-0ee6-464c-ade7-1e42486c97bc",
   "lastEditTime": 1766502299679
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import urllib.parse\n",
    "import requests\n",
    "import math\n",
    "import heapq\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from functools import lru_cache\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.window import Window\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DecimalType\n",
    ")\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9ba8499c-bcce-45cc-a2dc-7615438c43e4",
   "metadata": {
    "language": "python",
    "name": "constants"
   },
   "outputs": [],
   "source": [
    "INTEGRATION_NAME = 'DTI_FUEL'\n",
    "\n",
    "PCMILER_API_KEY = os.environ[\"PCMILER_API_KEY\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "SAMSARA_API_KEY = os.environ[\"SAMSARA_API_KEY\"]\n",
    "\n",
    "ROUTE_REPORTS_BASE_URL = 'https://pcmiler.alk.com/apis/rest/v1.0/Service.svc/route/routeReports'\n",
    "AVOIDFAVOR_BASE_URL     = 'https://fleets.trimblemaps.com/api/assets/v1/avoidFavorSets'\n",
    "VEHICLE_GROUP_AF_URL     = AVOIDFAVOR_BASE_URL + '/vehicleGroups'\n",
    "\n",
    "DTI_VEHICLE_GROUP_ID    = '1543010'\n",
    "DTI_DEFAULT_CLOSURE_ID  = '2104'\n",
    "DEFAULT_PROFILE_ID      = '15073630'\n",
    "\n",
    "METERS_PER_MILE = 1609.344\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'\n",
    "MAX_TOKENS = 10000\n",
    "\n",
    "STATE_TO_ABBR = {\n",
    "        \"Alabama\":\"AL\",\"Alaska\":\"AK\",\"Arizona\":\"AZ\",\"Arkansas\":\"AR\",\"California\":\"CA\",\"Colorado\":\"CO\",\n",
    "        \"Connecticut\":\"CT\",\"Delaware\":\"DE\",\"Florida\":\"FL\",\"Georgia\":\"GA\",\"Hawaii\":\"HI\",\"Idaho\":\"ID\",\n",
    "        \"Illinois\":\"IL\",\"Indiana\":\"IN\",\"Iowa\":\"IA\",\"Kansas\":\"KS\",\"Kentucky\":\"KY\",\"Louisiana\":\"LA\",\n",
    "        \"Maine\":\"ME\",\"Maryland\":\"MD\",\"Massachusetts\":\"MA\",\"Michigan\":\"MI\",\"Minnesota\":\"MN\",\"Mississippi\":\"MS\",\n",
    "        \"Missouri\":\"MO\",\"Montana\":\"MT\",\"Nebraska\":\"NE\",\"Nevada\":\"NV\",\"New Hampshire\":\"NH\",\"New Jersey\":\"NJ\",\n",
    "        \"New Mexico\":\"NM\",\"New York\":\"NY\",\"North Carolina\":\"NC\",\"North Dakota\":\"ND\",\"Ohio\":\"OH\",\n",
    "        \"Oklahoma\":\"OK\",\"Oregon\":\"OR\",\"Pennsylvania\":\"PA\",\"Rhode Island\":\"RI\",\"South Carolina\":\"SC\",\n",
    "        \"South Dakota\":\"SD\",\"Tennessee\":\"TN\",\"Texas\":\"TX\",\"Utah\":\"UT\",\"Vermont\":\"VT\",\"Virginia\":\"VA\",\n",
    "        \"Washington\":\"WA\",\"West Virginia\":\"WV\",\"Wisconsin\":\"WI\",\"Wyoming\":\"WY\",\"District of Columbia\":\"DC\",\n",
    "    }\n",
    "\n",
    "BASE = \"https://api.samsara.com\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {SAMSARA_API_KEY}\"}"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2d901cc-0c61-4867-a6a7-0742a9afba88",
   "metadata": {
    "language": "python",
    "name": "debug"
   },
   "outputs": [],
   "source": "DEBUG_PCM = os.environ.get(\"DEBUG_PCM\", \"0\") in (\"1\", \"true\", \"True\")\n\ndef _debug(msg):\n    if DEBUG_PCM:\n        print(f\"[PCM-DEBUG] {msg}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8935315-4a76-4a9d-9c40-00d954f8012a",
   "metadata": {
    "language": "python",
    "name": "import_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "try:\n    # Load in Wex Fuel data summary\n    DTI_FUEL_DATA_SUMMARY_SP = session.table(\"DEV_DOT_MLAI.DTI_FUEL.WEX_FUEL_DATA_SUMMARY\")\n    \n    # Load in DC Fuel Data\n    DC_FUEL_DATA_SP = session.table(\"DEV_DOT_MLAI.DTI_FUEL.DTI_HOMEFUEL_CURRENT_PRICE\")\n    \nexcept Exception as error:\n    print(f\"loading data failed: {error}\")\n    send_error_email(body=f'{error}')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fdebe5d3-81b0-4caa-abf9-31ddfb88e91e",
   "metadata": {
    "language": "python",
    "name": "error_email_function",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def send_error_email(body, subject='DTI FUEL ERROR', recipients='joe.haenel@dotfoods.com'):\n    session.call(\n        \"SYSTEM$SEND_EMAIL\", # Stored procedure name\n        INTEGRATION_NAME,\n        recipients, \n        subject, # email_subject\n        body # email_content\n    )\n    return True\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92c7ed40-1c9c-4aee-9b9e-3244c0aaa15a",
   "metadata": {
    "language": "python",
    "name": "geotunnel_sample"
   },
   "outputs": [],
   "source": "def sample_geotunnel(points, step_mi: float = 1.0):\n    \"\"\"Resample a polyline [(lon,lat),...] into ~1-mile spaced points with mile markers.\"\"\"\n    out = []\n    if not points: \n        return out\n    from math import radians, sin, cos, asin, sqrt\n    def hav_mi(lat1, lon1, lat2, lon2):\n        R = 3958.7613\n        dlat = radians(lat2-lat1); dlon = radians(lon2-lon1)\n        a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n        return 2*R*asin(sqrt(a))\n\n    acc = 0.0\n    lon0, lat0 = points[0]\n    out.append((lon0, lat0, 0.0))\n    carry = 0.0\n    for (lon1, lat1) in points[1:]:\n        seg = hav_mi(lat0, lon0, lat1, lon1)\n        while carry + seg >= step_mi and seg > 0:\n            t = (step_mi - carry) / seg\n            lon = lon0 + t*(lon1-lon0)\n            lat = lat0 + t*(lat1-lat0)\n            acc += step_mi\n            out.append((lon, lat, acc))\n            lon0, lat0 = lon, lat\n            seg -= (step_mi - carry)\n            carry = 0.0\n        carry += seg\n        lon0, lat0 = lon1, lat1\n    return out",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "388f96b9-32ce-4773-af86-56e990a5eed1",
   "metadata": {
    "language": "python",
    "name": "multiple_drivers_check_for_hos"
   },
   "outputs": [],
   "source": "def is_team_driving(stops_sp) -> bool:\n    \"\"\"\n    Team = both driver numbers present and non-zero on the first stop.\n    Works with '', None, or '0' coming from Snowflake.\n    \"\"\"\n    first = (\n        stops_sp.sort(F.col(\"LOAD_STOP_SEQUENCE_NO\").asc())\n                .select(\"DRIVER_ONE_EMP_NO\", \"DRIVER_TWO_EMP_NO\")\n                .limit(1)\n                .to_pandas()\n    )\n    if first.empty:\n        return False\n    d1 = str(first.iloc[0].get(\"DRIVER_ONE_EMP_NO\") or \"\").strip()\n    d2 = str(first.iloc[0].get(\"DRIVER_TWO_EMP_NO\") or \"\").strip()\n    def _has(val: str) -> bool:\n        try:\n            # treat numeric > 0 as present; ignore blanks/None/'0'\n            return int(float(val)) > 0\n        except Exception:\n            return len(val) > 0\n    return _has(d1) and _has(d2)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6943113-be68-4aed-9a6f-aced76086a57",
   "metadata": {
    "language": "python",
    "name": "expand_for_dcs_from_geotunnel"
   },
   "outputs": [],
   "source": "def expand_dc_candidates_from_geotunnel(\n    session,\n    routes_with_geotunnel_pd,   # pandas DF from build_routes_df_for_events(...)\n    *,\n    radius_mi: float = 5.0,\n    step_mi: float = 1.0,\n    dot_table: str = \"DEV_DOT_MLAI.DTI_FUEL.DTI_DOT_LOCATIONS\",\n) -> \"pd.DataFrame\":\n    \"\"\"\n    For each leg, sample the geotunnel, find nearby DOTs within `radius_mi`,\n    dedupe to the nearest per (LOAD_NO, FROM_SEQ, TO_SEQ, DOT_NAME),\n    and return DC candidate events with haversine distance (miles).\n\n    Debug logging:\n      - Set env DTI_FUEL_LOG_LEVEL (e.g., DEBUG, INFO) to control verbosity.\n      - Set env DTI_FUEL_EXPLAIN=1 to print query plans for logged steps.\n      - Set env DTI_FUEL_LOG_MAX_LEGS (default 5) to limit per-leg logging.\n      - We also set a session QUERY_TAG to correlate queries in Snowsight.\n\n    Tunables / guardrails (env):\n      - DTI_FUEL_MAX_SAMPLES (default 600): cap per-leg geotunnel samples.\n      - DTI_FUEL_MAX_CANDIDATES (default 5_000_000): skip legs with exploding\n        bbox-join candidate counts.\n\n    Requires utilities in this module:\n      - sample_geotunnel(points, step_mi)\n      - _polyline_length_mi(points)\n    \"\"\"\n    import os, uuid, time, traceback, math\n    import pandas as pd\n    from snowflake.snowpark import functions as F\n    from snowflake.snowpark.window import Window  # kept for compatibility (not used now)\n\n    # --------------------------- logging helpers ----------------------------\n    import logging\n    LOG_LEVEL = os.getenv(\"DTI_FUEL_LOG_LEVEL\", \"INFO\").upper()\n    EXPLAIN   = os.getenv(\"DTI_FUEL_EXPLAIN\", \"0\") == \"1\"\n    LOG_MAX_LEGS = int(os.getenv(\"DTI_FUEL_LOG_MAX_LEGS\", \"5\"))\n\n    MAX_SAMPLES = int(os.getenv(\"DTI_FUEL_MAX_SAMPLES\", \"600\"))\n    MAX_CANDIDATES = int(os.getenv(\"DTI_FUEL_MAX_CANDIDATES\", \"5000000\"))\n\n    logger = logging.getLogger(\"dti_fuel.expand_dc_candidates_from_geotunnel\")\n    if not logger.handlers:\n        _h = logging.StreamHandler()\n        _h.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n        logger.addHandler(_h)\n    logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))\n\n    run_id = str(uuid.uuid4())[:8]\n    try:\n        session.sql(f\"alter session set query_tag = 'dti_fuel:expand_dc:{run_id}'\").collect()\n    except Exception:\n        # Non-fatal: logging only\n        logger.debug(f\"[{run_id}] Could not set query_tag: {traceback.format_exc()}\")\n\n    def _last_qid():\n        try:\n            return session.sql(\"select last_query_id()\").collect()[0][0]\n        except Exception:\n            return None\n\n    def log_shape(df, label: str, do_explain: bool = False):\n        \"\"\"Executes COUNT to materialize, logs row count + last query id; optional EXPLAIN.\"\"\"\n        try:\n            t0 = time.perf_counter()\n            n = df.count()\n            qid = _last_qid()\n            dt = (time.perf_counter() - t0) * 1000.0\n            logger.info(f\"[{run_id}] {label}: rows={n} ({dt:.1f} ms){' qid='+qid if qid else ''}\")\n            if do_explain:\n                logger.debug(f\"[{run_id}] {label}: EXPLAIN start\")\n                df.explain()  # prints to stdout; captured by Snowflake logs\n                logger.debug(f\"[{run_id}] {label}: EXPLAIN end\")\n            return n\n        except Exception:\n            logger.error(f\"[{run_id}] {label}: count/explain failed\\n{traceback.format_exc()}\")\n            return None\n\n    METERS_PER_MILE = 1609.344\n    radius_m = float(radius_mi) * METERS_PER_MILE\n\n    # ---- DOT table prep (once) ----------------------------------------------\n    try:\n        d_raw = session.table(dot_table)\n        d_cols = [f.name for f in d_raw.schema.fields]\n        d_cols_u = [c.upper() for c in d_cols]\n        logger.info(f\"[{run_id}] DOT table '{dot_table}' columns={d_cols_u}\")\n    except Exception:\n        logger.error(f\"[{run_id}] Failed to open DOT table {dot_table}\\n{traceback.format_exc()}\")\n        raise\n\n    # Resolve LAT/LON (support LONGITUDE/LATITUDE fallbacks)\n    def _pick(df, cols_u, *names):\n        for n in names:\n            if n in cols_u:\n                return d_cols[cols_u.index(n)]\n        return None\n\n    d = d_raw\n    lon_name = _pick(d, d_cols_u, \"LON\", \"LONGITUDE\")\n    lat_name = _pick(d, d_cols_u, \"LAT\", \"LATITUDE\")\n    if lon_name is None or lat_name is None:\n        msg = f\"{dot_table} must contain LON/LAT (or LONGITUDE/LATITUDE). Got columns: {d_cols_u}\"\n        logger.error(f\"[{run_id}] {msg}\")\n        raise ValueError(msg)\n\n    if \"LON\" not in d_cols_u:\n        d = d.with_column(\"LON\", d[lon_name])\n    if \"LAT\" not in d_cols_u:\n        d = d.with_column(\"LAT\", d[lat_name])\n\n    # Guardrails (numeric only; avoid GEOGRAPHY entirely)\n    d = d.filter(\n        F.col(\"LAT\").is_not_null() & F.col(\"LON\").is_not_null() &\n        F.col(\"LAT\").between(-90, 90) & F.col(\"LON\").between(-180, 180)\n    )\n\n    # Add STATE if present (no polygon join fallback to avoid GEOGRAPHY ops)\n    d_cols = [f.name for f in d.schema.fields]\n    d_cols_u = [c.upper() for c in d_cols]\n    dot_state_col_u = next(\n        (c for c in [\"STATE\", \"STATE_ABBR\", \"STATE_ABBRV\", \"STATE_CODE\", \"ST\", \"STATEPROV\"] if c in d_cols_u),\n        None\n    )\n    if dot_state_col_u:\n        d_aug = d.with_column(\"STATE\", F.upper(d[dot_state_col_u]))\n    else:\n        d_aug = d.with_column(\"STATE\", F.lit(None))\n\n    # Ensure DOT_NAME exists; map common alternatives if needed\n    d_aug_cols   = [f.name for f in d_aug.schema.fields]\n    d_aug_cols_u = [c.upper() for c in d_aug_cols]\n    if \"DOT_NAME\" not in d_aug_cols_u:\n        alt_name_u = next(\n            (c for c in [\"DOT\", \"DOTNAME\", \"NAME\", \"LOCATION_NAME\", \"DOT_LOCATION_NAME\"] if c in d_aug_cols_u),\n            None\n        )\n        if not alt_name_u:\n            msg = f\"{dot_table} must contain DOT_NAME (or DOT/DOTNAME/NAME/LOCATION_NAME). Columns: {d_aug_cols_u}\"\n            logger.error(f\"[{run_id}] {msg}\")\n            raise ValueError(msg)\n        d_aug = d_aug.with_column(\"DOT_NAME\", d_aug[alt_name_u])\n\n    # Slim to only needed columns BEFORE any joins (no GEOGRAPHY columns)\n    d_aug = d_aug.select(\"DOT_NAME\", \"STATE\", \"LON\", \"LAT\")\n    log_shape(d_aug, \"d_aug (DOT catalog)\", do_explain=EXPLAIN)\n\n    # ---- Per-leg processing ---------------------------------------------------\n    out_chunks = []\n    leg_idx = 0\n\n    for r in routes_with_geotunnel_pd.itertuples(index=False):\n        leg_idx += 1\n        log_this_leg = leg_idx <= LOG_MAX_LEGS  # limit verbose logging\n\n        try:\n            load_no = int(getattr(r, \"LOAD_NO\"))\n            from_seq = int(getattr(r, \"LOAD_STOP_SEQUENCE_NO\"))\n            to_seq   = from_seq + 1\n            pts      = getattr(r, \"GEOTUNNEL_POINTS\", None)\n        except Exception:\n            logger.error(f\"[{run_id}] Leg[{leg_idx}] row parse failed\\n{traceback.format_exc()}\")\n            continue\n\n        if not pts:\n            if log_this_leg:\n                logger.info(f\"[{run_id}] Leg[{leg_idx}] load={load_no} seq={from_seq}->{to_seq}: no geotunnel points; skipping\")\n            continue\n\n        # per-leg mile-marker scaling\n        leg_pcm = getattr(r, \"LEG_MI_PCM\", None)\n        scale   = getattr(r, \"LEG_SCALE\", None)\n        if scale is None:\n            try:\n                gt_len = _polyline_length_mi(pts)\n                scale  = (float(leg_pcm)/gt_len) if (leg_pcm and gt_len) else 1.0\n            except Exception:\n                logger.error(f\"[{run_id}] Leg[{leg_idx}] scale compute failed\\n{traceback.format_exc()}\")\n                scale = 1.0\n\n        # avoid hopping over nearby DCs\n        step = min(float(step_mi), max(0.25, float(radius_mi)/2.0))\n        try:\n            samples = sample_geotunnel(pts, step_mi=step)\n        except Exception:\n            logger.error(f\"[{run_id}] Leg[{leg_idx}] sample_geotunnel failed\\n{traceback.format_exc()}\")\n            continue\n\n        if not samples:\n            if log_this_leg:\n                logger.info(f\"[{run_id}] Leg[{leg_idx}] load={load_no} seq={from_seq}->{to_seq}: 0 samples after step={step}\")\n            continue\n\n        # ---- Cap samples on very long legs (guardrail) ----------------------\n        if MAX_SAMPLES and len(samples) > MAX_SAMPLES:\n            stride = math.ceil(len(samples) / MAX_SAMPLES)\n            samples = samples[::stride]\n            if log_this_leg:\n                logger.info(f\"[{run_id}] Leg[{leg_idx}] capped samples to {len(samples)} (stride={stride})\")\n\n        # ---- Compute a per-leg bounding box & prefilter DOTs ----------------\n        leg_lats = [lat for (_, lat, _) in samples]\n        leg_lons = [lon for (lon, _, _) in samples]\n        mid_lat  = sum(leg_lats) / len(leg_lats)\n\n        pad_deg_lat = (radius_m / 111_320.0) * 1.5  # 1.5x safety pad\n        pad_deg_lon = pad_deg_lat / max(1e-6, math.cos(math.radians(mid_lat)))\n\n        min_lat, max_lat = min(leg_lats)-pad_deg_lat, max(leg_lats)+pad_deg_lat\n        min_lon, max_lon = min(leg_lons)-pad_deg_lon, max(leg_lons)+pad_deg_lon\n\n        d_leg = d_aug.filter(\n            (F.col(\"LAT\").between(min_lat, max_lat)) &\n            (F.col(\"LON\").between(min_lon, max_lon))\n        )\n        if log_this_leg:\n            log_shape(d_leg, f\"Leg[{leg_idx}] d_leg (prefiltered DOTs)\", do_explain=False)\n\n        # ---- Build tiny per-leg DataFrame (NO GEOGRAPHY) --------------------\n        leg_rows = [\n            (load_no, from_seq, to_seq, idx, float(lon), float(lat), float(mm) * float(scale))\n            for idx, (lon, lat, mm) in enumerate(samples)\n        ]\n        pts_df = session.create_dataframe(\n            leg_rows, schema=[\"LOAD_NO\",\"FROM_SEQ\",\"TO_SEQ\",\"PT_IDX\",\"LON\",\"LAT\",\"MM\"]\n        ).filter(\n            F.col(\"LAT\").is_not_null() & F.col(\"LON\").is_not_null()\n        )\n\n        if log_this_leg:\n            logger.info(f\"[{run_id}] Leg[{leg_idx}] load={load_no} {from_seq}->{to_seq}: samples={len(leg_rows)}\")\n            log_shape(pts_df, f\"Leg[{leg_idx}] pts_df\", do_explain=False)\n\n        # ---------- Numeric bounding box join (per-leg reduced) --------------\n        p  = pts_df.alias(\"P\")\n        d2 = d_leg.alias(\"D\")\n\n        lat_delta_deg = F.lit(radius_m / 111320.0)  # ~meters per degree latitude\n        lon_den       = F.greatest(F.lit(1e-6), F.call_function(\"COS\", F.call_function(\"RADIANS\", p[\"LAT\"])))\n        lon_delta_deg = F.lit(radius_m / 111320.0) / lon_den\n\n        cond_bb = (F.abs(d2[\"LAT\"] - p[\"LAT\"]) <= lat_delta_deg) & (F.abs(d2[\"LON\"] - p[\"LON\"]) <= lon_delta_deg)\n        cand_bb = p.join(d2, cond_bb, how=\"inner\")\n        cand_cnt = log_shape(cand_bb, f\"Leg[{leg_idx}] cand_bb (bbox join)\", do_explain=EXPLAIN)\n\n        # Guardrail: skip legs that still explode\n        if cand_cnt is not None and MAX_CANDIDATES and cand_cnt > MAX_CANDIDATES:\n            logger.warning(f\"[{run_id}] Leg[{leg_idx}] too many candidates ({cand_cnt} > {MAX_CANDIDATES}); skipping leg\")\n            continue\n\n        # ---------- Core candidate columns (still no GEOGRAPHY) -------------\n        cand_core = cand_bb.select(\n            p[\"LOAD_NO\"].alias(\"LOAD_NO\"),\n            p[\"FROM_SEQ\"].alias(\"FROM_SEQ\"),\n            p[\"TO_SEQ\"].alias(\"TO_SEQ\"),\n            d2[\"DOT_NAME\"].alias(\"DOT_NAME\"),\n            d2[\"STATE\"].alias(\"STATE\"),\n            p[\"MM\"].alias(\"MM\"),\n            p[\"LAT\"].alias(\"PT_LAT\"),   p[\"LON\"].alias(\"PT_LON\"),\n            d2[\"LAT\"].alias(\"DOT_LAT\"), d2[\"LON\"].alias(\"DOT_LON\"),\n        )\n        if log_this_leg:\n            log_shape(cand_core, f\"Leg[{leg_idx}] cand_core\", do_explain=False)\n\n        # ---------- Haversine approx for distance (meters) -------------------\n        R = 6371008.8  # meters\n        dlat = F.call_function(\"RADIANS\", F.col(\"DOT_LAT\") - F.col(\"PT_LAT\"))\n        dlon = F.call_function(\"RADIANS\", F.col(\"DOT_LON\") - F.col(\"PT_LON\"))\n        phi1 = F.call_function(\"RADIANS\", F.col(\"PT_LAT\"))\n        phi2 = F.call_function(\"RADIANS\", F.col(\"DOT_LAT\"))\n        sin_dlat = F.call_function(\"SIN\", dlat / F.lit(2.0))\n        sin_dlon = F.call_function(\"SIN\", dlon / F.lit(2.0))\n        a = sin_dlat * sin_dlat + F.call_function(\"COS\", phi1) * F.call_function(\"COS\", phi2) * sin_dlon * sin_dlon\n        c = F.call_function(\"ASIN\", F.call_function(\"SQRT\", a)) * F.lit(2.0)\n        cand_hav = cand_core.with_column(\"D_APPROX_M\", F.lit(R) * c)\n        if log_this_leg:\n            log_shape(cand_hav, f\"Leg[{leg_idx}] cand_hav (+D_APPROX_M)\", do_explain=False)\n\n        # ---------- EARLY radius filter to shrink data -----------------------\n        within = F.col(\"D_APPROX_M\") <= F.lit(radius_m)\n        cand_within = cand_hav.filter(within)\n        if log_this_leg:\n            log_shape(cand_within, f\"Leg[{leg_idx}] cand_within (<= radius)\", do_explain=False)\n\n        # ---------- Dedupe using aggregation (cheaper than window) -----------\n        keys = [\"LOAD_NO\", \"FROM_SEQ\", \"TO_SEQ\", \"DOT_NAME\"]\n        mins = (\n            cand_within.group_by(*keys)\n            .agg(F.min(F.col(\"D_APPROX_M\")).alias(\"D_MIN\"))\n        )\n        cw = cand_within.alias(\"CW\")\n        m  = mins.alias(\"M\")\n        \n        nearest = (\n            cw.join(\n                m,\n                (cw[\"LOAD_NO\"]  == m[\"LOAD_NO\"])  &\n                (cw[\"FROM_SEQ\"] == m[\"FROM_SEQ\"]) &\n                (cw[\"TO_SEQ\"]   == m[\"TO_SEQ\"])   &\n                (cw[\"DOT_NAME\"] == m[\"DOT_NAME\"]) &\n                (cw[\"D_APPROX_M\"] == m[\"D_MIN\"]),\n                how=\"inner\",\n            )\n            # ---- restore original column names so downstream .select() works ----\n            .select(*[cw[c].alias(c) for c in cw.columns])\n        )\n\n        if log_this_leg:\n            log_shape(nearest, f\"Leg[{leg_idx}] nearest (agg-based)\", do_explain=EXPLAIN)\n\n        # ---------- Finalize projection --------------------------------------\n        final_df = nearest.select(\n            \"LOAD_NO\",\"FROM_SEQ\",\"TO_SEQ\",\"STATE\",\n            F.col(\"DOT_LON\").alias(\"LON\"),\n            F.col(\"DOT_LAT\").alias(\"LAT\"),\n            F.col(\"MM\").alias(\"MM_APPROX_MI\"),\n            (F.col(\"D_APPROX_M\") / F.lit(METERS_PER_MILE)).alias(\"DIST_MI\"),\n            \"DOT_NAME\",\n        )\n\n\n        if log_this_leg:\n            log_shape(final_df, f\"Leg[{leg_idx}] final_df (within radius)\", do_explain=EXPLAIN)\n\n        try:\n            out_pd = final_df.to_pandas()\n        except Exception:\n            logger.error(f\"[{run_id}] Leg[{leg_idx}] to_pandas failed (see incident in Query History)\\n{traceback.format_exc()}\")\n            # force execution separately to capture qid even if pandas bridge failed\n            try:\n                _ = final_df.count()\n                logger.info(f\"[{run_id}] Leg[{leg_idx}] final_df.count() succeeded; qid={_last_qid()}\")\n            except Exception:\n                logger.error(f\"[{run_id}] Leg[{leg_idx}] final_df.count() also failed; qid={_last_qid()}\")\n            raise\n\n        if not out_pd.empty:\n            out_pd.sort_values([\"LOAD_NO\",\"FROM_SEQ\",\"TO_SEQ\",\"MM_APPROX_MI\"], inplace=True)\n            out_pd[\"EVENT_TYPE\"] = \"DC\"\n            out_pd[\"IS_DC\"] = True\n            out_chunks.append(out_pd)\n        elif log_this_leg:\n            logger.info(f\"[{run_id}] Leg[{leg_idx}] final: 0 rows within {radius_mi} mi\")\n\n    if out_chunks:\n        result = pd.concat(out_chunks, ignore_index=True)\n        logger.info(f\"[{run_id}] DONE: concatenated chunks -> {len(result)} rows\")\n        return result\n\n    logger.info(f\"[{run_id}] DONE: no DC candidates produced\")\n    # empty but schema-consistent when no matches\n    return pd.DataFrame(columns=[\n        \"LOAD_NO\",\"FROM_SEQ\",\"TO_SEQ\",\"STATE\",\"LON\",\"LAT\",\n        \"MM_APPROX_MI\",\"DIST_MI\",\"DOT_NAME\",\"EVENT_TYPE\",\"IS_DC\"\n    ])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24a67f72-8f47-4546-b176-bee34fbbbc8e",
   "metadata": {
    "language": "python",
    "name": "Parse_Input_Variables"
   },
   "outputs": [],
   "source": "def _coerce_payload_to_dict(obj):\n    \"\"\"Return a Python dict for the payload regardless of how Snowpark returns VARIANT.\"\"\"\n    if isinstance(obj, dict):\n        return obj\n    if isinstance(obj, str):\n        return json.loads(obj)\n    try:\n        return json.loads(json.dumps(obj))\n    except Exception:\n        raise TypeError(f\"Expected dict-like payload; got {type(obj).__name__}\")\n\ndef _dequote_once(s: str) -> str:\n    if not s: return s\n    s = s.strip()\n    if len(s) >= 2 and s[0] == s[-1] and s[0] in (\"'\", '\"'):\n        return s[1:-1]\n    return s\n\ndef _undollar(s: str) -> str:\n    if s.startswith(\"$$\") and s.endswith(\"$$\") and len(s) >= 4:\n        return s[2:-2]\n    return s\n\ndef _normalize_args(argv):\n    out = []\n    for raw in argv or []:\n        s = str(raw)\n        s = _dequote_once(_undollar(s)).strip()\n        if s: out.append(s)\n    return out\n\n_RUNID_RE = re.compile(r\"^\\s*RUN_ID\\s*=\\s*(.+?)\\s*$\", re.IGNORECASE)\n\ndef _extract_run_id(argv):\n    for s in _normalize_args(argv):\n        m = _RUNID_RE.match(s)\n        if m:\n            return m.group(1).strip()\n    return None\n\ndef _extract_run_json(argv):\n    args = _normalize_args(argv)\n    # RUN_JSON={run_json}\n    for s in args:\n        if s.upper().startswith(\"RUN_JSON=\"):\n            return s.split(\"=\", 1)[1].strip()\n    # Single positional JSON\n    for s in args:\n        if s.startswith(\"{\") and s.endswith(\"}\"):\n            return s\n    return None\n\ndef _get_run_payload(argv, session):\n    args = _normalize_args(argv)\n    rid  = _extract_run_id(args)\n    if rid:\n        rows = (session.table(\"DEV_DOT_MLAI.DTI_FUEL.FUEL_RUN_INBOX\")\n                      .filter(col(\"RUN_ID\") == rid)\n                      .select(\"PAYLOAD\")\n                      .limit(1)\n                      .collect())\n        if not rows:\n            raise ValueError(f\"RUN_ID '{rid}' not found in DEV_DOT_MLAI.DTI_FUEL.FUEL_RUN_INBOX\")\n        return _coerce_payload_to_dict(rows[0][0])\n\n    j = _extract_run_json(args)\n    if j:\n        return json.loads(j)\n\n    raise ValueError(\"No RUN_ID or JSON argument found. argv(normalized)=\" + repr(args))\n\n\ndef loads_json_or_none(s):\n    if not s or s in (\"None\", \"\"): return None\n    if isinstance(s, dict): return s\n    return json.loads(s)\n\ndef get_num(v, typ, default):\n    try:\n        if v in (None, \"\", \"None\"): return default\n        return typ(v)\n    except Exception:\n        return default\n\ndef parse_ts(s):\n    if not s or s in (\"None\", \"\"): return None\n    return datetime.fromisoformat(s.replace(\" \", \"T\").replace(\"Z\", \"\"))\n\nSTOP_SCHEMA = StructType([\n    StructField(\"LOAD_NUMBER\", IntegerType()),\n    StructField(\"LOAD_STOP_SEQUENCE_NO\", IntegerType()),\n    StructField(\"APPT_ETA_TS\", TimestampType()),\n    StructField(\"STOP_NAME\", StringType()),\n    StructField(\"ADDRESS1\", StringType()),\n    StructField(\"CITY\", StringType()),\n    StructField(\"STATE\", StringType()),\n    StructField(\"POSTALCODE\", StringType()),\n    StructField(\"LON\", DoubleType()),\n    StructField(\"LAT\", DoubleType()),\n    StructField(\"DRIVER_ONE_EMP_NO\", IntegerType()),\n    StructField(\"DRIVER_TWO_EMP_NO\", IntegerType()),\n    StructField(\"DOT_DOMICILEABBREVIATION\", StringType()),\n])\n\n@dataclass(frozen=True)\nclass FuelPlanInputs:\n    load_number: int\n    initial_fuel: float\n    tank_capacity: float\n    mpg: float\n    safety_buffer: float\n    stops_df: Optional[Any] = None\n    raw_payload: Any = None       \n    run_id: str = \"\"              \n\ndef build_inputs_from_argv(argv: List[str], session: Session) -> FuelPlanInputs:\n    run = _get_run_payload(argv, session)\n    rid = _extract_run_id(argv) or uuid.uuid4().hex\n\n    load_number   = get_num(run.get(\"LOAD_NUMBER\"), int, None)\n    initial_fuel  = get_num(run.get(\"INITIAL_FUEL\"), float, None)\n    tank_capacity = get_num(run.get(\"TANK_CAPACITY\"), float, None)\n    mpg           = get_num(run.get(\"MPG\"), float, None)\n    safety_buffer = get_num(run.get(\"SAFETY_BUFFER\"), float, None)\n\n    missing = [k for k, v in {\n        \"LOAD_NUMBER\": load_number,\n        \"INITIAL_FUEL\": initial_fuel,\n        \"TANK_CAPACITY\": tank_capacity,\n        \"MPG\": mpg,\n        \"SAFETY_BUFFER\": safety_buffer,\n    }.items() if v is None]\n    if missing:\n        raise ValueError(f\"RUN_JSON missing required run-level field(s): {', '.join(missing)}\")\n\n    stops_in = run.get(\"STOPS\")\n    if not isinstance(stops_in, list) or not stops_in:\n        raise ValueError(\"RUN_JSON must contain a non-empty STOPS array.\")\n    if len(stops_in) < 2:\n        raise ValueError(\"STOPS must contain at least an origin and a destination.\")\n\n    norm = []\n    for s in stops_in:\n        seq = get_num(s.get(\"LOAD_STOP_SEQUENCE_NO\"), int, 0)\n        norm.append({\n            \"LOAD_NUMBER\":        load_number,\n            \"LOAD_STOP_SEQUENCE_NO\": seq,\n            \"APPT_ETA_TS\":        parse_ts(s.get(\"LOAD_STOP_SCHEDULEDSTARTDATE\")),\n            \"STOP_NAME\":          (s.get(\"DOT_DOMICILENAME\") or s.get(\"STOP_NAME\") or \"\"),\n            \"ADDRESS1\":           (s.get(\"ADDRESSONE\") or s.get(\"ADDRESS1\") or \"\"),\n            \"CITY\":               (s.get(\"CITY\") or \"\"),\n            \"STATE\":              (s.get(\"STATE\") or \"\"),\n            \"POSTALCODE\":         (s.get(\"POSTALCODE\") or \"\"),\n            \"LON\":                get_num(s.get(\"LON\"), float, None),\n            \"LAT\":                get_num(s.get(\"LAT\"), float, None),\n            \"DRIVER_ONE_EMP_NO\":  get_num(s.get(\"DRIVER_ONE_EMPLOYEE_NUMBER\"), int, 0),\n            \"DRIVER_TWO_EMP_NO\":  get_num(s.get(\"DRIVER_TWO_EMPLOYEE_NUMBER\"), int, 0),\n            \"DOT_DOMICILEABBREVIATION\": (s.get(\"DOT_DOMICILEABBREVIATION\") or \"\"),\n        })\n\n    norm.sort(key=lambda r: r[\"LOAD_STOP_SEQUENCE_NO\"])\n    stops_df = session.create_dataframe(norm, schema=STOP_SCHEMA)\n    \n    return FuelPlanInputs(\n        load_number=load_number,\n        initial_fuel=initial_fuel,\n        tank_capacity=tank_capacity,\n        mpg=mpg,\n        safety_buffer=safety_buffer,\n        stops_df=stops_df,\n        raw_payload=run,\n        run_id=rid\n    )\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e81aa558-1a13-42e2-ae79-6402a31e24fa",
   "metadata": {
    "language": "python",
    "name": "HOS_Retrieval_Logic"
   },
   "outputs": [],
   "source": "def _first(d, *names):\n    for n in names:\n        v = d.get(n)\n        if v is not None:\n            return v\n    return None\n\ndef get_hos_for_driver(driver_id: str) -> dict:\n    r = requests.get(f\"{BASE}/fleet/hos/clocks\",\n                     params={\"driverIds\": driver_id, \"limit\": 1},\n                     headers=HEADERS, timeout=20)\n    r.raise_for_status()\n    data = r.json().get(\"data\", [])\n    if not data:\n        return {}\n\n    rec = data[0]\n    drive_s = _first(rec, \"availableDriveSeconds\", \"remainingDriveSeconds\")\n    if drive_s is None:\n        ms = _first(rec, \"driveRemainingMs\", \"availableDriveMs\", \"remainingDriveMs\")\n        drive_s = ms / 1000.0 if ms is not None else None\n\n    shift_s = _first(rec, \"availableShiftSeconds\", \"remainingShiftSeconds\")\n    if shift_s is None:\n        ms = _first(rec, \"shiftRemainingMs\", \"availableShiftMs\", \"remainingShiftMs\")\n        shift_s = ms / 1000.0 if ms is not None else None\n\n    cycle_s = _first(rec, \"availableCycleSeconds\", \"remainingCycleSeconds\", \"cycleRemainingMs\")\n    if isinstance(cycle_s, (int, float)) and cycle_s > 10000 and \"Ms\" in \"\".join(rec.keys()):\n        # crude guard if cycle_s actually came back in ms\n        cycle_s = cycle_s / 1000.0\n\n    return {\n        \"drive_hours\": None if drive_s is None else round(drive_s / 3600.0, 2),\n        \"shift_hours\": None if shift_s is None else round(shift_s / 3600.0, 2),\n        \"cycle_hours\": None if cycle_s is None else round(cycle_s / 3600.0, 2),\n        \"raw\": rec,\n    }\n\n_EMPNO_RX = re.compile(r\"\\((\\d+)\\)\\s*$\")\n\ndef _parse_empno_from_name(name: str) -> str | None:\n    if not name:\n        return None\n    m = _EMPNO_RX.search(str(name))\n    return m.group(1) if m else None\n\ndef find_driver_id_by_empno(emp_no: int | str) -> str | None:\n    \"\"\"Iterate /fleet/drivers and match '(empno)' at the end of the name.\"\"\"\n    emp_no = str(emp_no).strip()\n    after = None\n    for _ in range(100):  # hard cap to avoid runaway loops\n        params = {\"limit\": 200}\n        if after:\n            params[\"after\"] = after\n        r = requests.get(f\"{BASE}/fleet/drivers\", params=params, headers=HEADERS, timeout=20)\n        r.raise_for_status()\n        j = r.json()\n        data = j.get(\"data\") or j.get(\"drivers\") or []\n        for d in data:\n            n = d.get(\"name\") or \"\"\n            parsed = _parse_empno_from_name(n)\n            if parsed == emp_no:\n                did = d.get(\"id\") or d.get(\"driverId\")\n                return str(did) if did is not None else None\n        pg = j.get(\"pagination\") or {}\n        has_next = bool(pg.get(\"hasNextPage\"))\n        after = pg.get(\"endCursor\")\n        if not has_next or not after:\n            break\n    return None\n\ndef get_start_drive_hours_for_load(session, stops_sp) -> float:\n    \"\"\"\n    Look at the first stop’s drivers (DRIVER_ONE_EMP_NO / DRIVER_TWO_EMP_NO),\n    map to Samsara driverId(s), fetch HOS clocks, and return remaining drive hours.\n    For team, we use the max of the two as a conservative “you can still drive” number.\n    Fallback to 10.0h if anything is missing.\n    \"\"\"\n    try:\n        first = (\n            stops_sp.sort(F.col(\"LOAD_STOP_SEQUENCE_NO\").asc())\n                    .select(\"DRIVER_ONE_EMP_NO\",\"DRIVER_TWO_EMP_NO\")\n                    .limit(1).to_pandas()\n        )\n        emp_candidates = []\n        if not first.empty:\n            d1 = first.iloc[0].get(\"DRIVER_ONE_EMP_NO\")\n            d2 = first.iloc[0].get(\"DRIVER_TWO_EMP_NO\")\n            for v in (d1, d2):\n                if v and int(v) > 0:\n                    emp_candidates.append(int(v))\n\n        hours = []\n        for emp in emp_candidates:\n            did = find_driver_id_by_empno(emp)\n            if did:\n                hos = get_hos_for_driver(did)\n                if hos and hos.get(\"drive_hours\") is not None:\n                    hours.append(float(hos[\"drive_hours\"]))\n        if hours:\n            # For solo this is just the solo’s hours; for team take the max.\n            return round(max(hours), 2)\n    except Exception as e:\n        print(f\"[HOS] Falling back to default start_drive_hours; reason: {e}\")\n    return 10.0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "058f4634-d825-49c9-99f4-b83db3f88f86",
   "metadata": {
    "language": "python",
    "name": "Preview_DTI_FUEL_DATA",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "DTI_FUEL_DATA_SUMMARY_SP.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e9d3bcc-845f-4b68-91a8-caf9322469ed",
   "metadata": {
    "language": "python",
    "name": "Preview_DC_FUEL_DATA",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "DC_FUEL_DATA_SP.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0923c6e5-7d18-49ad-94eb-6b3945607524",
   "metadata": {
    "language": "python",
    "name": "removing_tax_from_dc",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "dc   = DC_FUEL_DATA_SP.alias(\"dc\")\nsumm = DTI_FUEL_DATA_SUMMARY_SP.select(\"STATE\", \"STATE_TAX_MODE\").alias(\"summ\")\n\n# Join on STATE and compute net-of-tax price\ndc_fuel_with_mode = (\n    dc.join(summ, dc[\"STATE\"] == summ[\"STATE\"], how=\"left\")\n      .with_column(\n          \"PPG_NO_TAX\",\n          dc[\"PRICEPERGALLON\"].cast(\"FLOAT\")\n          - F.coalesce(summ[\"STATE_TAX_MODE\"].cast(\"FLOAT\"), F.lit(0.0))\n      )\n      .select(\n          *[dc[c] for c in dc.columns],\n          summ[\"STATE_TAX_MODE\"],\n          F.col(\"PPG_NO_TAX\")\n      )\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4fdcb12-d35e-4865-975e-0b118b5467be",
   "metadata": {
    "language": "python",
    "name": "preview_dc_fuel_no_tax",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "dc_fuel_with_mode.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28d31675-d4ba-45f1-a3bb-3d97b297f8e7",
   "metadata": {
    "language": "python",
    "name": "Authenticate_For_Samsara"
   },
   "outputs": [],
   "source": "def authenticate_fleet(api_key: str, account_id=None) -> str:\n    url = \"https://fleets.trimblemaps.com/api/assets/v1/accounts/authenticate\"\n    payload = {\"apiKey\": api_key}\n    if account_id:\n        payload[\"accountId\"] = account_id\n    resp = requests.post(url, json=payload, timeout=15)\n    resp.raise_for_status()\n    token = resp.json().get(\"token\")\n    if not token:\n        raise RuntimeError(f\"Identity returned 200 but no token: {resp.text}\")\n    return token",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2125472-c33e-4e23-9ee6-9b0942202cc8",
   "metadata": {
    "language": "python",
    "name": "get_avoid_favor_sets"
   },
   "outputs": [],
   "source": "def fetch_avoid_favor_sets(api_key: str, use_vehicle_group: bool = False) -> list[str]:\n    jwt = authenticate_fleet(api_key)\n    headers = {\"Authorization\": f\"Bearer {jwt}\"}\n    url = f\"{AVOIDFAVOR_BASE_URL}?limit=1000&includeDefaultClosure=true\"\n    resp = requests.get(url, headers=headers, timeout=15)\n    resp.raise_for_status()\n    all_sets = resp.json().get(\"data\", [])\n    prod_sets = [\n        str(s[\"setId\"])\n        for s in all_sets\n        if any(g[\"id\"] == int(DTI_VEHICLE_GROUP_ID) for g in s.get(\"vehicleGroups\", []))\n    ]\n    if DTI_DEFAULT_CLOSURE_ID not in prod_sets:\n        prod_sets.append(DTI_DEFAULT_CLOSURE_ID)\n    return prod_sets",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b83e9c9d-f14f-4d02-9c17-bc18dd8b867f",
   "metadata": {
    "language": "python",
    "name": "get_mileage_for_route"
   },
   "outputs": [],
   "source": "def get_route_mileage(f_lon, f_lat, t_lon, t_lat, api_key, af_set_ids):\n    params = {\n        \"stops\":     f\"{f_lon},{f_lat};{t_lon},{t_lat}\",\n        \"afSetIDs\":  \",\".join(af_set_ids),\n        \"reports\":   \"CalcMiles\",\n        \"profileId\": DEFAULT_PROFILE_ID,\n        \"authToken\": api_key\n    }\n    url = f\"{ROUTE_REPORTS_BASE_URL}?{urllib.parse.urlencode(params)}\"\n    last_exc = None\n    for attempt in range(1, 4):\n        try:\n            r = requests.get(url, timeout=30)\n            r.raise_for_status()\n            j = r.json()\n            return float(j[0][\"TMiles\"])\n        except Exception as e:\n            print(f\"[CalcMiles] attempt {attempt} failed url={url} err={e} body={getattr(r, 'text', '')[:500]}\")\n            last_exc = e\n            time.sleep(1)\n    raise RuntimeError(f\"PC*MILER CalcMiles failed after retries: {last_exc}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1089ae4-6651-4df4-86f8-afc561627d7a",
   "metadata": {
    "language": "python",
    "name": "mileage_cache"
   },
   "outputs": [],
   "source": "def create_mileage_cache(api_key: str):\n    # Pull AF set IDs once per kernel; memoize route calls\n    af_ids = tuple(fetch_avoid_favor_sets(api_key))\n    @lru_cache(maxsize=8192)\n    def _pcm(lon_a: float, lat_a: float, lon_b: float, lat_b: float) -> float:\n        return get_route_mileage(lon_a, lat_a, lon_b, lat_b, api_key, af_ids)\n    return _pcm",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ebe9156-fc63-4c50-953e-1ed182e567f3",
   "metadata": {
    "language": "python",
    "name": "build_legs_and_get_mileage"
   },
   "outputs": [],
   "source": "def build_legs_and_compute_mileage(session: Session, stops_df, load_number: int):\n    \"\"\"\n    Input: Snowpark DF (stops_df) with columns at least:\n      LOAD_NUMBER (int), LOAD_STOP_SEQUENCE_NO (int), LON (float), LAT (float)\n\n    Output:\n      - Snowpark DF with legs and mileage\n      - If persist=True, writes TMP_LEG_MILES_<load_number> and returns (df, table_name)\n    \"\"\"\n    from snowflake.snowpark.types import StructType, StructField, IntegerType, DoubleType\n    \n    cols = {c.upper() for c in stops_df.columns}\n    required = {\"LOAD_NUMBER\",\"LOAD_STOP_SEQUENCE_NO\",\"LON\",\"LAT\"}\n    missing = required - cols\n    if missing:\n        raise ValueError(f\"stops_df missing required columns: {sorted(missing)}\")\n\n    pdf = (stops_df\n           .select(\"LOAD_NUMBER\",\"LOAD_STOP_SEQUENCE_NO\",\"LON\",\"LAT\")\n           .to_pandas()\n           .sort_values([\"LOAD_NUMBER\",\"LOAD_STOP_SEQUENCE_NO\"])\n           .reset_index(drop=True))\n\n    # Prime mileage function\n    if not PC_MILER_API_KEY:\n        raise RuntimeError(\"PC_MILER_API_KEY not found (env or Snowflake secret).\")\n    mileage_fn = create_mileage_cache(PC_MILER_API_KEY)\n\n    # Build legs and compute miles\n    legs = []\n    for load_no, grp in pdf.groupby(\"LOAD_NUMBER\", sort=False):\n        grp = grp.sort_values(\"LOAD_STOP_SEQUENCE_NO\").reset_index(drop=True)\n        for i in range(len(grp) - 1):\n            a = grp.iloc[i]; b = grp.iloc[i+1]\n            lon_a, lat_a, lon_b, lat_b = a[\"LON\"], a[\"LAT\"], b[\"LON\"], b[\"LAT\"]\n            miles = None\n            if pd.notna(lon_a) and pd.notna(lat_a) and pd.notna(lon_b) and pd.notna(lat_b):\n                try:\n                    miles = mileage_fn(float(lon_a), float(lat_a), float(lon_b), float(lat_b))\n                except Exception as e:\n                    print(f\"[LegError] {load_no} {a['LOAD_STOP_SEQUENCE_NO']}→{b['LOAD_STOP_SEQUENCE_NO']} :: {e}\")\n                    miles = None\n            legs.append({\n                \"LOAD_NUMBER\": int(load_no),\n                \"FROM_SEQ\":    int(a[\"LOAD_STOP_SEQUENCE_NO\"]),\n                \"TO_SEQ\":      int(b[\"LOAD_STOP_SEQUENCE_NO\"]),\n                \"FROM_LON\":    float(lon_a) if pd.notna(lon_a) else None,\n                \"FROM_LAT\":    float(lat_a) if pd.notna(lat_a) else None,\n                \"TO_LON\":      float(lon_b) if pd.notna(lon_b) else None,\n                \"TO_LAT\":      float(lat_b) if pd.notna(lat_b) else None,\n                \"MILES_PCMILER\": miles\n            })\n\n    schema = StructType([\n        StructField(\"LOAD_NUMBER\", IntegerType()),\n        StructField(\"FROM_SEQ\", IntegerType()),\n        StructField(\"TO_SEQ\", IntegerType()),\n        StructField(\"FROM_LON\", DoubleType()),\n        StructField(\"FROM_LAT\", DoubleType()),\n        StructField(\"TO_LON\", DoubleType()),\n        StructField(\"TO_LAT\", DoubleType()),\n        StructField(\"MILES_PCMILER\", DoubleType()),\n    ])\n    \n    rows = [\n        (\n            r[\"LOAD_NUMBER\"], r[\"FROM_SEQ\"], r[\"TO_SEQ\"],\n            r[\"FROM_LON\"], r[\"FROM_LAT\"], r[\"TO_LON\"], r[\"TO_LAT\"],\n            r[\"MILES_PCMILER\"]\n        )\n        for r in legs\n    ]\n    \n    legs_sp = session.create_dataframe(rows, schema=schema)\n    return legs_sp",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acfb5fe1-f370-4264-9867-0e8d434772a6",
   "metadata": {
    "language": "python",
    "name": "load_in_dot_locations"
   },
   "outputs": [],
   "source": "def _load_dot_locations_dict(session) -> dict[str, str]:\n    rows = (session.table(\"DEV_DOT_MLAI.DTI_FUEL.DTI_DOT_LOCATIONS\")\n                 .select(\"DOT_NAME\",\"LAT\",\"LON\")\n                 .collect())\n    # dict: {\"Name\": \"lat, lon\"}\n    return {r[\"DOT_NAME\"]: f\"{float(r['LAT'])}, {float(r['LON'])}\" for r in rows}",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3f1f00b-fe9d-48d1-9b26-7a261a6760f3",
   "metadata": {
    "language": "python",
    "name": "correlate_points_to_states"
   },
   "outputs": [],
   "source": "def _classify_points_to_states(session, points: list[tuple[float, float]]):\n    if not points:\n        return []\n    values_sql = \" , \".join([f\"({i},{lon},{lat})\" for i,(lon,lat) in enumerate(points)])\n    q = f\"\"\"\n    WITH pts(seq, lon, lat) AS (\n      SELECT * FROM VALUES {values_sql}\n    )\n    SELECT p.seq, p.lon, p.lat, s.name AS state_name\n    FROM pts p\n    LEFT JOIN DEV_DOT_MLAI.DTI_FUEL.STATES_RAW s\n      ON ST_CONTAINS(s.GEOG, TO_GEOGRAPHY(ST_POINT(p.lon, p.lat)))\n    ORDER BY p.seq\n    \"\"\"\n    df = session.sql(q).to_pandas()\n    return list(df[\"STATE_NAME\"].fillna(\"UNKNOWN\"))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "205a5fce-d4b1-48bc-b915-eea49dc98da7",
   "metadata": {
    "language": "python",
    "name": "get_geotunnel_pcmiler"
   },
   "outputs": [],
   "source": "def get_route_geotunnel(f_lon, f_lat, t_lon, t_lat, api_key):\n    \"\"\"\n    Return a list of (lon, lat) tuples along the routed path between A->B.\n    \"\"\"\n    try:\n        af_ids = fetch_avoid_favor_sets(api_key)\n    except Exception:\n        af_ids = []\n\n    params = {\n        \"stops\":     f\"{f_lon},{f_lat};{t_lon},{t_lat}\",\n        \"reports\":   \"RoutePath\",\n        \"profileId\": DEFAULT_PROFILE_ID,\n        \"authToken\": api_key\n    }\n    if af_ids:\n        # BUGFIX: add the dot before join + coerce to str\n        params[\"afSetIDs\"] = \",\".join(map(str, af_ids))\n\n    url = f\"{ROUTE_REPORTS_BASE_URL}?{urllib.parse.urlencode(params)}\"\n\n    last_exc = None\n    for attempt in range(1, 4):\n        t0 = time.time()\n        try:\n            _debug(f\"RoutePath attempt={attempt} url={url}\")\n            r = requests.get(url, timeout=30)\n            _debug(f\"status={r.status_code} ct={r.headers.get('content-type')} t_ms={(time.time()-t0)*1000:.1f} len={len(r.text)}\")\n            r.raise_for_status()\n            j = r.json()\n\n                        # Normalize top node\n            top = j[0] if isinstance(j, list) and j else (j if isinstance(j, dict) else {})\n            keys = list(top.keys()) if isinstance(top, dict) else []\n            _debug(f\"top_type={type(top).__name__} keys={keys[:20]}\")\n\n            #handle GeoJSON-style payloads\n            pts_raw = None\n            if isinstance(top, dict):\n                geom = top.get(\"geometry\")\n                if isinstance(geom, dict):\n                    gtype = (geom.get(\"type\") or \"\").lower()\n                    coords = geom.get(\"coordinates\")\n                    if coords:\n                        # GeoJSON is [lon, lat]\n                        if gtype == \"linestring\":\n                            pts_raw = coords  # [[lon, lat], ...]\n                            _debug(f\"geometry=LineString coords_len={len(coords)}\")\n                        elif gtype == \"multilinestring\":\n                            # Flatten list of lines\n                            pts_raw = [pt for line in coords for pt in line]\n                            _debug(f\"geometry=MultiLineString segments={len(coords)} total_pts={len(pts_raw)}\")\n\n            if pts_raw is None and isinstance(top, dict):\n                for k in (\"Path\", \"Shape\", \"Polyline\", \"Coords\"):\n                    v = top.get(k)\n                    if v:\n                        _debug(f\"legacy candidate {k} length={len(v)} first2={str(v[:2])[:200]}\")\n                        pts_raw = [{\"Lon\": p.get(\"Lon\") or p.get(\"Longitude\"),\n                                    \"Lat\": p.get(\"Lat\") or p.get(\"Latitude\")} for p in v]\n                        break\n\n            if pts_raw is None:\n                raise RuntimeError(f\"GeoTunnel empty; top_keys={keys[:20]}\")\n\n            # Canonicalize to list[(lon, lat)]\n            out = []\n            # If GeoJSON (list[list[2 floats]]), normalize\n            if pts_raw and isinstance(pts_raw[0], (list, tuple)):\n                for lon, lat in pts_raw:\n                    if lon is not None and lat is not None:\n                        out.append((float(lon), float(lat)))\n            else:\n                # Legacy dict form with Lon/Lat keys\n                for p in pts_raw:\n                    lon = p.get(\"Lon\")\n                    lat = p.get(\"Lat\")\n                    if lon is not None and lat is not None:\n                        out.append((float(lon), float(lat)))\n\n            _debug(f\"parsed_points={len(out)} f=({f_lon},{f_lat}) t=({t_lon},{t_lat})\")\n            if not out:\n                raise RuntimeError(f\"GeoTunnel parsed 0 points; top_keys={keys[:20]}\")\n            return out\n\n\n        except Exception as e:\n            preview = \"\"\n            try:\n                preview = r.text[:500]\n            except Exception:\n                pass\n            _debug(f\"[RoutePath] attempt={attempt} failed err={e} body={preview}\")\n            last_exc = e\n            time.sleep(1)\n\n    raise RuntimeError(f\"PC*MILER RoutePath failed after retries: {last_exc}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48bd5443-cb73-4775-afa4-47eb39eb8cf2",
   "metadata": {
    "language": "python",
    "name": "normalize_and_make_transitions"
   },
   "outputs": [],
   "source": "def _norm_state(s: str) -> str:\n    if not s: return \"\"\n    return STATE_TO_ABBR.get(s, s).upper()\n\ndef _make_transitions(points: list[tuple[float, float]], states: list[str]):\n    from math import radians, sin, cos, asin, sqrt\n    def hav_mi(lat1, lon1, lat2, lon2):\n        R = 3958.7613\n        dlat = radians(lat2-lat1); dlon = radians(lon2-lon1)\n        a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n        return 2*R*asin(sqrt(a))\n\n    out = []\n    if not points or not states:\n        return out\n\n    # cumulative miles along the tunnel\n    mm = 0.0\n    prev_state = _norm_state(states[0])\n    prev_lon, prev_lat = points[0]\n    for i in range(1, len(points)):\n        lon, lat = points[i]\n        mm += hav_mi(prev_lat, prev_lon, lat, lon)\n        cur_state = _norm_state(states[i])\n        if cur_state and prev_state and cur_state != prev_state:\n            out.append({\n                \"from_state\": prev_state,\n                \"to_state\": cur_state,\n                \"transition_lon\": float(lon),\n                \"transition_lat\": float(lat),\n                \"mm\": float(mm), # mile marker\n            })\n        prev_state = cur_state\n        prev_lon, prev_lat = lon, lat\n    return out\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd7a052c-cf9c-4186-9f39-cde6fd5f61fd",
   "metadata": {
    "language": "python",
    "name": "build_route_events_df"
   },
   "outputs": [],
   "source": "def build_routes_df_for_events(session, stops_sp, legs_sp, pcmiler_api_key: str) -> \"pd.DataFrame\":\n    # Normalize stops (unchanged)\n    stops_pd = (\n        stops_sp.select(\n            F.col(\"LOAD_NUMBER\").alias(\"LOAD_NO\"),\n            F.col(\"LOAD_STOP_SEQUENCE_NO\").alias(\"LOAD_STOP_SEQUENCE_NO\"),\n            \"CITY\",\"STATE\",\n            F.col(\"LON\").alias(\"LON\"),\n            F.col(\"LAT\").alias(\"LAT\"),\n            F.col(\"DOT_DOMICILEABBREVIATION\").alias(\"DOT_DOMICILE_ABBRV\")\n        ).to_pandas()\n    )\n    stops_pd.columns = [str(c).strip().upper() for c in stops_pd.columns]\n    print(\"stops_pd columns:\", list(stops_pd.columns))\n    stops_pd = stops_pd.sort_values([\"LOAD_NO\",\"LOAD_STOP_SEQUENCE_NO\"]).reset_index(drop=True)\n\n    # Init per-leg fields\n    stops_pd[\"GEOTUNNEL_POINTS\"]  = None\n    stops_pd[\"STATE_TRANSITIONS\"] = None\n    # carry per-leg scaling info for downstream DC sampling\n    stops_pd[\"LEG_MI_PCM\"] = None\n    stops_pd[\"LEG_SCALE\"]  = None\n\n    # Bring leg miles\n    legs_pd = (\n        legs_sp.select(\"LOAD_NUMBER\",\"FROM_SEQ\",\"TO_SEQ\",\"FROM_LON\",\"FROM_LAT\",\"TO_LON\",\"TO_LAT\",\"MILES_PCMILER\")\n               .to_pandas()\n               .sort_values([\"LOAD_NUMBER\",\"FROM_SEQ\"])\n               .reset_index(drop=True)\n    )\n    leg_mi_map = {\n        (int(r.LOAD_NUMBER), int(r.FROM_SEQ)): (float(r.MILES_PCMILER) if pd.notna(r.MILES_PCMILER) else None)\n        for r in legs_pd.itertuples(index=False)\n    }\n\n    for r in legs_pd.itertuples(index=False):\n        load_no  = int(r.LOAD_NUMBER)\n        from_seq = int(r.FROM_SEQ)\n        tunnel = get_route_geotunnel(\n            f_lon=float(r.FROM_LON), f_lat=float(r.FROM_LAT),\n            t_lon=float(r.TO_LON),   t_lat=float(r.TO_LAT),\n            api_key=pcmiler_api_key\n        )\n        # Classify states + build transitions with raw mm\n        states = _classify_points_to_states(session, tunnel)\n        transitions = _make_transitions(tunnel, states)  # has 'mm' relative to leg start\n\n        # scale leg mm to PC*Miler miles\n        leg_pcm_mi = leg_mi_map.get((load_no, from_seq))\n        gt_len_mi  = _polyline_length_mi(tunnel)\n        scale = (float(leg_pcm_mi) / gt_len_mi) if (leg_pcm_mi and gt_len_mi) else 1.0\n        for t in transitions:\n            if \"mm\" in t and t[\"mm\"] is not None:\n                t[\"mm\"] = float(t[\"mm\"]) * float(scale)\n\n        if DEBUG_PCM:\n            print(f\"[PCM-DEBUG] load={load_no} leg={from_seq}->{int(r.TO_SEQ)} \"\n                  f\"pts={len(tunnel)} trans={len(transitions)} scale={scale:.6f}\")\n\n        match = (stops_pd[\"LOAD_NO\"] == load_no) & (stops_pd[\"LOAD_STOP_SEQUENCE_NO\"] == from_seq)\n        if match.any():\n            idx = stops_pd.index[match][0]\n            stops_pd.at[idx, \"GEOTUNNEL_POINTS\"]  = tunnel\n            stops_pd.at[idx, \"STATE_TRANSITIONS\"] = transitions\n            stops_pd.at[idx, \"LEG_MI_PCM\"]        = leg_pcm_mi\n            stops_pd.at[idx, \"LEG_SCALE\"]         = scale\n\n    return stops_pd\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fee2a0f-49b1-4589-a637-ed1c236ddaac",
   "metadata": {
    "language": "python",
    "name": "process_routes_df"
   },
   "outputs": [],
   "source": "def process_routes(\n    session: Session,\n    legs_sp, # DF: LOAD_NUMBER, FROM_SEQ, TO_SEQ, FROM_LON, FROM_LAT, TO_LON, TO_LAT, MILES_PCMILER\n    *,\n    sample_km: float = 10.0,\n    max_samples_per_leg: int = 500,\n    states_table: str = \"DEV_DOT_MLAI.DTI_FUEL.STATES_RAW\",\n    dot_table: str = \"DEV_DOT_MLAI.DTI_FUEL.DTI_DOT_LOCATIONS\",\n):\n    import uuid\n    # unwrap (df, table_name)\n    if isinstance(legs_sp, tuple):\n        legs_sp = legs_sp[0]\n\n    tmp_legs = f\"TEMP_LEGS_{uuid.uuid4().hex[:12]}\"\n    legs_sp.write.mode(\"overwrite\").save_as_table(tmp_legs, table_type=\"temporary\")\n\n    q = f\"\"\"\n    WITH\n    legs AS (\n      SELECT\n        LOAD_NUMBER, FROM_SEQ, TO_SEQ,\n        FROM_LON, FROM_LAT, TO_LON, TO_LAT, MILES_PCMILER,\n        ST_MAKELINE(\n          TO_GEOGRAPHY(ST_POINT(FROM_LON, FROM_LAT)),\n          TO_GEOGRAPHY(ST_POINT(TO_LON, TO_LAT))\n        ) AS GEOG_LINE\n      FROM {tmp_legs}\n    ),\n    legs_sized AS (\n      SELECT\n        *,\n        ST_LENGTH(GEOG_LINE) AS LINE_M,\n        LEAST(\n          {max_samples_per_leg},\n          GREATEST(2, FLOOR(ST_LENGTH(GEOG_LINE) / ({sample_km}*1000)) + 1)\n        ) AS N_SAMPLES\n      FROM legs\n    ),\n    -- GENERATOR must be constant; generate 0..max-1 and keep < N_SAMPLES per leg\n    gen AS (SELECT SEQ4() AS G FROM TABLE(GENERATOR(ROWCOUNT => {max_samples_per_leg}))),\n    samples AS (\n      SELECT\n        l.LOAD_NUMBER, l.FROM_SEQ, l.TO_SEQ,\n        l.FROM_LON, l.FROM_LAT, l.TO_LON, l.TO_LAT,\n        l.MILES_PCMILER, l.GEOG_LINE, l.LINE_M, l.N_SAMPLES,\n        g.G AS IDX\n      FROM legs_sized l\n      JOIN gen g ON g.G < l.N_SAMPLES\n    ),\n    pts AS (\n      SELECT\n        LOAD_NUMBER, FROM_SEQ, TO_SEQ, MILES_PCMILER, GEOG_LINE, LINE_M, N_SAMPLES, IDX,\n        (IDX::DOUBLE) / NULLIF(N_SAMPLES - 1, 0) AS FRACTION,\n        -- replace ST_GEODESICINTERPOLATE/LINE_INTERPOLATE with simple lon/lat lerp\n        TO_GEOGRAPHY(\n          ST_POINT(\n            FROM_LON + ((TO_LON - FROM_LON) * ((IDX::DOUBLE) / NULLIF(N_SAMPLES - 1, 0))),\n            FROM_LAT + ((TO_LAT - FROM_LAT) * ((IDX::DOUBLE) / NULLIF(N_SAMPLES - 1, 0)))\n          )\n        ) AS PT,\n        ((IDX::DOUBLE) / NULLIF(N_SAMPLES - 1, 0)) * LINE_M AS PT_M\n      FROM samples\n    ),\n    pts_state AS (\n      SELECT p.*, s.NAME AS STATE_NAME\n      FROM pts p\n      LEFT JOIN {states_table} s\n        ON ST_CONTAINS(s.GEOG, p.PT)\n    ),\n    -- two-step window to avoid nested window error\n    runs1 AS (\n      SELECT\n        *,\n        COALESCE(STATE_NAME, 'UNKNOWN') AS STATE_NM_NN,\n        LAG(COALESCE(STATE_NAME,'UNKNOWN')) OVER (\n          PARTITION BY LOAD_NUMBER, FROM_SEQ, TO_SEQ\n          ORDER BY IDX\n        ) AS STATE_PREV\n      FROM pts_state\n    ),\n    runs2 AS (\n      SELECT\n        *,\n        CASE WHEN STATE_NM_NN <> STATE_PREV THEN 1 ELSE 0 END AS CHG\n      FROM runs1\n    ),\n    runs AS (\n      SELECT\n        *,\n        SUM(CHG) OVER (\n          PARTITION BY LOAD_NUMBER, FROM_SEQ, TO_SEQ\n          ORDER BY IDX\n          ROWS UNBOUNDED PRECEDING\n        ) AS RUN_ID\n      FROM runs2\n    ),\n    segs AS (\n      SELECT\n        LOAD_NUMBER, FROM_SEQ, TO_SEQ,\n        STATE_NM_NN AS STATE,\n        MIN(PT_M)  AS START_M,\n        MAX(PT_M)  AS END_M,\n        MIN(IDX)   AS START_IDX,\n        MAX(IDX)   AS END_IDX,\n        MAX(LINE_M) AS LINE_M\n      FROM runs\n      GROUP BY LOAD_NUMBER, FROM_SEQ, TO_SEQ, STATE_NM_NN, RUN_ID\n    ),\n    segs_miles AS (\n      SELECT s.*, (END_M - START_M) / {METERS_PER_MILE} AS SEG_MI\n      FROM segs s\n    ),\n    segs_scaled AS (\n      SELECT\n        sm.*,\n        l.MILES_PCMILER,\n        CASE\n          WHEN l.MILES_PCMILER IS NOT NULL AND l.MILES_PCMILER > 0 THEN\n            ROUND(\n              sm.SEG_MI * l.MILES_PCMILER\n              / NULLIF(SUM(sm.SEG_MI) OVER (PARTITION BY sm.LOAD_NUMBER, sm.FROM_SEQ, sm.TO_SEQ), 0),\n              2\n            )\n          ELSE ROUND(sm.SEG_MI, 2)\n        END AS SEG_MI_SCALED\n      FROM segs_miles sm\n      JOIN {tmp_legs} l\n        ON sm.LOAD_NUMBER = l.LOAD_NUMBER\n       AND sm.FROM_SEQ    = l.FROM_SEQ\n       AND sm.TO_SEQ      = l.TO_SEQ\n    ),\n    ends AS (\n      SELECT\n        l.LOAD_NUMBER, l.FROM_SEQ, l.TO_SEQ,\n        TO_GEOGRAPHY(ST_POINT(l.FROM_LON, l.FROM_LAT)) AS FROM_GEOG,\n        TO_GEOGRAPHY(ST_POINT(l.TO_LON,   l.TO_LAT))   AS TO_GEOG\n      FROM {tmp_legs} l\n    ),\n    nearest_from AS (\n      SELECT\n        e.LOAD_NUMBER, e.FROM_SEQ, e.TO_SEQ,\n        d.DOT_NAME,\n        ROW_NUMBER() OVER (\n          PARTITION BY e.LOAD_NUMBER, e.FROM_SEQ, e.TO_SEQ\n          ORDER BY ST_DISTANCE(e.FROM_GEOG, d.GEOG)\n        ) AS RN\n      FROM ends e\n      JOIN {dot_table} d\n        ON ST_DISTANCE(e.FROM_GEOG, d.GEOG) <= 50000\n    ),\n    nearest_to AS (\n      SELECT\n        e.LOAD_NUMBER, e.FROM_SEQ, e.TO_SEQ,\n        d.DOT_NAME,\n        ROW_NUMBER() OVER (\n          PARTITION BY e.LOAD_NUMBER, e.FROM_SEQ, e.TO_SEQ\n          ORDER BY ST_DISTANCE(e.TO_GEOG, d.GEOG)\n        ) AS RN\n      FROM ends e\n      JOIN {dot_table} d\n        ON ST_DISTANCE(e.TO_GEOG, d.GEOG) <= 50000\n    ),\n    labeled AS (\n      SELECT\n        ss.LOAD_NUMBER, ss.FROM_SEQ, ss.TO_SEQ,\n        ss.STATE,\n        ss.START_IDX, ss.END_IDX, ss.START_M, ss.END_M,\n        ss.SEG_MI_SCALED, ss.MILES_PCMILER,\n        nf.DOT_NAME AS FROM_DOT,\n        nt.DOT_NAME AS TO_DOT\n      FROM segs_scaled ss\n      LEFT JOIN (SELECT * FROM nearest_from WHERE RN = 1) nf\n        ON ss.LOAD_NUMBER = nf.LOAD_NUMBER AND ss.FROM_SEQ = nf.FROM_SEQ AND ss.TO_SEQ = nf.TO_SEQ\n      LEFT JOIN (SELECT * FROM nearest_to   WHERE RN = 1) nt\n        ON ss.LOAD_NUMBER = nt.LOAD_NUMBER AND ss.FROM_SEQ = nt.FROM_SEQ AND ss.TO_SEQ = nt.TO_SEQ\n      ORDER BY LOAD_NUMBER, FROM_SEQ, TO_SEQ, START_IDX\n    )\n    SELECT * FROM labeled\n    \"\"\"\n    return session.sql(q)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "209c40e7-aa30-4cbf-b202-f46db0681e3d",
   "metadata": {
    "language": "python",
    "name": "expand_route_events_df"
   },
   "outputs": [],
   "source": "def expand_route_events(\n    session,\n    legs_sp, # DF: LOAD_NUMBER, FROM_SEQ, TO_SEQ, FROM_LON, FROM_LAT, TO_LON, TO_LAT\n    *,\n    threshold_miles: float = 15.0,\n    max_samples_per_leg: int = 500,\n    dot_table: str = \"DEV_DOT_MLAI.DTI_FUEL.DTI_DOT_LOCATIONS\",\n):\n    \"\"\"\n    Returns Snowpark DF with columns expected by add_event_miles:\n      LOAD_NO, EVENT_SEQ, LON, LAT, DIST_MI, FROM_SEQ, TO_SEQ, DOT_NAME\n    \"\"\"\n    import uuid\n    tmp_legs = f\"TEMP_LEGS_{uuid.uuid4().hex[:12]}\"\n    legs_sp.write.mode(\"overwrite\").save_as_table(tmp_legs, table_type=\"temporary\")\n\n    max_m = threshold_miles * METERS_PER_MILE\n\n    q = f\"\"\"\n    WITH\n    legs AS (\n      SELECT\n        LOAD_NUMBER, FROM_SEQ, TO_SEQ,\n        FROM_LON, FROM_LAT, TO_LON, TO_LAT\n      FROM {tmp_legs}\n    ),\n    legs_sized AS (\n      SELECT\n        *,\n        -- estimate polyline length (straight line) in meters\n        ST_LENGTH(\n          ST_MAKELINE(\n            TO_GEOGRAPHY(ST_POINT(FROM_LON, FROM_LAT)),\n            TO_GEOGRAPHY(ST_POINT(TO_LON, TO_LAT))\n          )\n        ) AS LINE_M,\n        -- choose sample count (capped) so we can “walk” the line\n        LEAST(\n          {max_samples_per_leg},\n          GREATEST(2, FLOOR(\n            ST_LENGTH(ST_MAKELINE(TO_GEOGRAPHY(ST_POINT(FROM_LON, FROM_LAT)),\n                                  TO_GEOGRAPHY(ST_POINT(TO_LON, TO_LAT)))) / (10*1000)\n          ) + 1)\n        ) AS N_SAMPLES\n      FROM legs\n    ),\n    gen AS (SELECT SEQ4() AS G FROM TABLE(GENERATOR(ROWCOUNT => {max_samples_per_leg}))),\n    samples AS (\n      SELECT\n        l.LOAD_NUMBER, l.FROM_SEQ, l.TO_SEQ,\n        l.FROM_LON, l.FROM_LAT, l.TO_LON, l.TO_LAT,\n        l.LINE_M, l.N_SAMPLES,\n        g.G AS IDX,\n        (g.G::DOUBLE) / NULLIF(l.N_SAMPLES - 1, 0) AS FRAC\n      FROM legs_sized l\n      JOIN gen g ON g.G < l.N_SAMPLES\n    ),\n    pts AS (\n      SELECT\n        LOAD_NUMBER, FROM_SEQ, TO_SEQ,\n        -- interpolate lon/lat between endpoints (straight-line lerp)\n        TO_GEOGRAPHY(\n          ST_POINT(\n            FROM_LON + ((TO_LON - FROM_LON) * FRAC),\n            FROM_LAT + ((TO_LAT - FROM_LAT) * FRAC)\n          )\n        ) AS PT,\n        FRAC * LINE_M AS PT_M\n      FROM samples\n    ),\n    hits AS (\n      SELECT\n        p.LOAD_NUMBER, p.FROM_SEQ, p.TO_SEQ,\n        d.DOT_NAME, d.LAT, d.LON,\n        ST_DISTANCE(p.PT, d.GEOG) AS D_M,\n        p.PT_M\n      FROM pts p\n      JOIN {dot_table} d\n        ON ST_DISTANCE(p.PT, d.GEOG) <= {max_m}\n    ),\n    nearest_per_dot AS (\n      SELECT\n        *,\n        ROW_NUMBER() OVER (\n          PARTITION BY LOAD_NUMBER, FROM_SEQ, TO_SEQ, DOT_NAME\n          ORDER BY D_M ASC\n        ) AS RN\n      FROM hits\n    ),\n    events AS (\n      SELECT\n        LOAD_NUMBER AS LOAD_NO,\n        FROM_SEQ, TO_SEQ, DOT_NAME,\n        LAT, LON,\n        ROUND(D_M / {METERS_PER_MILE}, 2) AS DIST_MI,\n        PT_M\n      FROM nearest_per_dot\n      WHERE RN = 1\n    ),\n    numbered AS (\n      SELECT\n        LOAD_NO, FROM_SEQ, TO_SEQ, DOT_NAME, LAT, LON, DIST_MI,\n        ROW_NUMBER() OVER (\n          PARTITION BY LOAD_NO, FROM_SEQ, TO_SEQ\n          ORDER BY PT_M\n        ) AS EVENT_SEQ\n      FROM events\n    )\n    SELECT * FROM numbered\n    ORDER BY LOAD_NO, FROM_SEQ, TO_SEQ, EVENT_SEQ\n    \"\"\"\n    return session.sql(q)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2043312-e3be-424e-a9bd-5f3196e753dd",
   "metadata": {
    "language": "python",
    "name": "add_miles_to_events"
   },
   "outputs": [],
   "source": "def add_event_miles(events_df, mileage_fn):\n    \"\"\"\n    For each LOAD_NO, compute miles from each event to the next event using `mileage_fn`\n    (your PC*MILER-backed function). Leaves the last event in each load with None.\n\n    Requires columns: LOAD_NO, EVENT_SEQ, LON, LAT\n    \"\"\"\n    import math\n    import pandas as pd\n\n    if events_df.empty:\n        return events_df\n\n    # Work on a sorted copy\n    df = events_df.sort_values([\"LOAD_NO\", \"EVENT_SEQ\"]).copy()\n    df[\"EVENT_MILES_TO_NEXT\"] = None\n\n    for _, grp in df.groupby(\"LOAD_NO\", sort=False):\n        idxs = grp.index.to_list()\n        for cur_idx, next_idx in zip(idxs[:-1], idxs[1:]):\n            lon_a = df.at[cur_idx, \"LON\"]\n            lat_a = df.at[cur_idx, \"LAT\"]\n            lon_b = df.at[next_idx, \"LON\"]\n            lat_b = df.at[next_idx, \"LAT\"]\n\n            miles = None\n            # Only call the API when we have good coords\n            if (pd.notna(lon_a) and pd.notna(lat_a) and\n                pd.notna(lon_b) and pd.notna(lat_b)):\n                try:\n                    # mileage_fn is your cached PCMILER function\n                    miles = float(mileage_fn(float(lon_a), float(lat_a),\n                                             float(lon_b), float(lat_b)))\n                except Exception:\n                    miles = None\n\n            df.at[cur_idx, \"EVENT_MILES_TO_NEXT\"] = miles\n\n    return df\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "305e1a52-baf5-4639-adc8-572a6020fe69",
   "metadata": {
    "language": "python",
    "name": "cleanup_event_miles"
   },
   "outputs": [],
   "source": "def reconcile_event_miles(events_df: \"pd.DataFrame\", tol_mi: float = 0.5) -> \"pd.DataFrame\":\n    \"\"\"\n    Ensure Σ EVENT_MILES_TO_NEXT aligns to last(MM_APPROX_MI) - first(MM_APPROX_MI), per LOAD_NO.\n    If the residual exceeds tol_mi, distribute proportionally across positive hops.\n    \"\"\"\n    if events_df is None or events_df.empty:\n        return events_df\n    df = events_df.copy()\n    df[\"EVENT_MILES_TO_NEXT\"] = pd.to_numeric(df[\"EVENT_MILES_TO_NEXT\"], errors=\"coerce\").fillna(0.0)\n\n    for load_no, g in df.groupby(\"LOAD_NO\", sort=False):\n        g = g.sort_values(\"EVENT_SEQ\")\n        mm_total = (pd.to_numeric(g[\"MM_APPROX_MI\"], errors=\"coerce\").bfill().fillna(0.0).iloc[-1]\n                    - pd.to_numeric(g[\"MM_APPROX_MI\"], errors=\"coerce\").bfill().fillna(0.0).iloc[0])\n        mi_sum   = float(g[\"EVENT_MILES_TO_NEXT\"].sum())\n        resid    = float(mm_total - mi_sum)\n        if abs(resid) <= float(tol_mi) or mi_sum <= 0.0:\n            continue\n        # proportional adjustment over positive hops\n        pos = g[\"EVENT_MILES_TO_NEXT\"] > 0\n        weight = g.loc[pos, \"EVENT_MILES_TO_NEXT\"].values\n        wsum   = weight.sum()\n        if wsum <= 0:\n            continue\n        adj = (weight / wsum) * resid\n        df.loc[g.loc[pos].index, \"EVENT_MILES_TO_NEXT\"] = (g.loc[pos, \"EVENT_MILES_TO_NEXT\"].values + adj).clip(min=0.0)\n    # small cleanup\n    df[\"EVENT_MILES_TO_NEXT\"] = df[\"EVENT_MILES_TO_NEXT\"].round(3)\n    return df\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc6430ad-f124-4ea7-a59c-35aed648bb27",
   "metadata": {
    "language": "python",
    "name": "build_price_data"
   },
   "outputs": [],
   "source": "def build_price_inputs(session):\n    summ_sp = session.table(\"DEV_DOT_MLAI.DTI_FUEL.WEX_FUEL_DATA_SUMMARY\")\n\n    # Key WEX medians by normalized 2-letter code (works for 'IL' and 'Illinois')\n    summ_pd = (\n        summ_sp.select(\n            F.col(\"STATE\").alias(\"STATE\"),\n            F.col(\"NET_EX_TAX_MEDIAN\").cast(\"FLOAT\").alias(\"NET_EX_TAX_MEDIAN\"),\n            F.col(\"STATE_TAX_MODE\").cast(\"FLOAT\").alias(\"STATE_TAX_MODE\"),\n        ).to_pandas()\n    )\n    state_price = {_state_key(str(r[\"STATE\"])): float(r[\"NET_EX_TAX_MEDIAN\"])\n                   for _, r in summ_pd.iterrows() if r[\"NET_EX_TAX_MEDIAN\"] is not None}\n\n    # DC prices, joined to WEX tax mode, with corrected Snowpark join signature\n    dc_sp   = session.table(\"DEV_DOT_MLAI.DTI_FUEL.DTI_HOMEFUEL_CURRENT_PRICE\")\n    dc_cols = {c.upper(): c for c in dc_sp.columns}\n    state_col = dc_cols.get(\"STATEDC\", dc_cols.get(\"STATE\"))\n    if not state_col:\n        raise ValueError(\"DTI_HOMEFUEL_CURRENT_PRICE missing STATE/STATEDC.\")\n\n    dc   = dc_sp.alias(\"dc\")\n    summ = (\n        summ_sp.select(F.col(\"STATE\").alias(\"S_STATE\"),\n                       F.col(\"STATE_TAX_MODE\").cast(\"FLOAT\").alias(\"STATE_TAX_MODE\"))\n               .alias(\"summ\")\n    )\n    \n    dc_joined = (\n        dc.join(\n            summ,\n            on=F.upper(dc[state_col]) == F.upper(summ[\"S_STATE\"]),\n            how=\"left\",\n        )\n        .select(\n            dc[\"STOPNUMBER\"].alias(\"STOPNUMBER\"),\n            dc[\"CITY\"].alias(\"CITY\"),\n            dc[state_col].alias(\"STATE\"),\n            dc[\"LATITUDE\"].cast(\"FLOAT\").alias(\"LAT\"),\n            dc[\"LONGITUDE\"].cast(\"FLOAT\").alias(\"LON\"),\n            dc[\"PRICEPERGALLON\"].cast(\"FLOAT\").alias(\"PPG_RAW\"),\n            (dc[\"PRICEPERGALLON\"].cast(\"FLOAT\") - F.coalesce(summ[\"STATE_TAX_MODE\"], F.lit(0.0))).alias(\"PPG_NO_TAX\"),\n        )\n    )\n\n    dc_fuel_df = dc_joined.to_pandas()\n    return dc_fuel_df, state_price\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82f01534-8f8c-46df-ac7f-f2931eb47425",
   "metadata": {
    "language": "python",
    "name": "haversine_miles"
   },
   "outputs": [],
   "source": "def _haversine_miles(lat1, lon1, lat2, lon2):\n    # great-circle distance in miles\n    R = 3958.7613\n    dlat = math.radians(lat2 - lat1)\n    dlon = math.radians(lon2 - lon1)\n    a = (math.sin(dlat/2)**2\n         + math.cos(math.radians(lat1))*math.cos(math.radians(lat2))*math.sin(dlon/2)**2)\n    return 2*R*math.asin(math.sqrt(a))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5142dbd9-7cb3-4e25-a7ea-4b3704962b41",
   "metadata": {
    "language": "python",
    "name": "append_price"
   },
   "outputs": [],
   "source": "def append_price_info(events_df: pd.DataFrame, dc_fuel_df: pd.DataFrame, state_price: dict, nearest_within_miles: float = 5.0) -> pd.DataFrame:\n    \"\"\"\n    For each event row (expects columns: STATE, LAT, LONG),\n    - find nearest DC station in the same STATE and attach its net-of-tax price (PPG_NO_TAX)\n    - also attach a WEX state fallback price (NET_EX_TAX_MEDIAN)\n    - choose EVENT_PPG as nearest DC price if within `nearest_within_miles`, else fallback to WEX\n    \"\"\"\n    if events_df.empty:\n        # still add the columns so downstream doesn’t break\n        out = events_df.copy()\n        for c in [\"NEAR_DC_STOPNUMBER\",\"NEAR_DC_CITY\",\"NEAR_DC_STATE\",\"NEAR_DC_PPG_NO_TAX\",\"NEAR_DC_DISTANCE_MI\",\"WEX_STATE_NET_EX_MEDIAN\",\"EVENT_PPG\"]:\n            out[c] = None\n        return out\n\n    # Normalize expected columns\n    need = {\"STATE\",\"LAT\",\"LON\"}\n    missing = [c for c in need if c not in events_df.columns]\n    if missing:\n        raise KeyError(f\"append_price_info: events_df missing {missing}. Has {list(events_df.columns)}\")\n\n    out = events_df.copy()\n    out[\"NEAR_DC_STOPNUMBER\"]      = None\n    out[\"NEAR_DC_CITY\"]            = None\n    out[\"NEAR_DC_STATE\"]           = None\n    out[\"NEAR_DC_PPG_NO_TAX\"]      = None\n    out[\"NEAR_DC_DISTANCE_MI\"]     = None\n    out[\"WEX_STATE_NET_EX_MEDIAN\"] = out[\"STATE\"].map(state_price).astype(float)\n\n    # Normalize event states\n    out[\"STATE_KEY\"] = out[\"STATE\"].map(_state_key)\n\n    # Normalize DC price states\n    dc_fuel_df = dc_fuel_df.copy()\n    dc_fuel_df[\"STATE_KEY\"] = dc_fuel_df[\"STATE\"].apply(_state_key)\n\n    # Index DCs by normalized key\n    dc_by_state = {st: grp.reset_index(drop=True)\n                   for st, grp in dc_fuel_df.groupby(\"STATE_KEY\", dropna=False)}\n\n    # Use STATE_KEY for nearest-DC lookup\n    for i, r in out.iterrows():\n        st = r[\"STATE_KEY\"]\n        lat = r[\"LAT\"]; lon = r[\"LON\"]\n        best_dist = None; best_idx = None; best_df = None\n\n        if st and st in dc_by_state and pd.notna(lat) and pd.notna(lon):\n            cand = dc_by_state[st]\n            # compute distances\n            dists = cand.apply(lambda rr: _haversine_miles(lat, lon, rr[\"LAT\"], rr[\"LON\"]), axis=1)\n            j = int(dists.idxmin()) if len(dists) else None\n            if j is not None:\n                best_dist = float(dists.loc[j])\n                best_idx  = j\n                best_df   = cand\n\n        if best_idx is not None:\n            out.at[i, \"NEAR_DC_STOPNUMBER\"]  = best_df.at[best_idx, \"STOPNUMBER\"]\n            out.at[i, \"NEAR_DC_CITY\"]        = best_df.at[best_idx, \"CITY\"]\n            out.at[i, \"NEAR_DC_STATE\"]       = best_df.at[best_idx, \"STATE\"]\n            out.at[i, \"NEAR_DC_PPG_NO_TAX\"]  = float(best_df.at[best_idx, \"PPG_NO_TAX\"]) if pd.notna(best_df.at[best_idx, \"PPG_NO_TAX\"]) else None\n            out.at[i, \"NEAR_DC_DISTANCE_MI\"] = best_dist\n\n        # Choose event price: nearest DC within threshold, else WEX state median\n        dc_price = out.at[i, \"NEAR_DC_PPG_NO_TAX\"]\n        wex_fallback = out.at[i, \"WEX_STATE_NET_EX_MEDIAN\"]\n        if dc_price is not None and (out.at[i, \"NEAR_DC_DISTANCE_MI\"] is None or out.at[i, \"NEAR_DC_DISTANCE_MI\"] <= nearest_within_miles):\n            out.at[i, \"EVENT_PPG\"] = float(dc_price)\n        else:\n            out.at[i, \"EVENT_PPG\"] = float(wex_fallback) if pd.notna(wex_fallback) else None\n\n    out[\"STATE_KEY\"] = out[\"STATE\"].map(_state_key)\n    out[\"WEX_STATE_NET_EX_MEDIAN\"] = out[\"STATE_KEY\"].map(state_price)\n\n    # only use a DC price when it's within the threshold\n    mask_near_dc = (\n        pd.notna(out[\"NEAR_DC_PPG_NO_TAX\"]) &\n        (out[\"NEAR_DC_DISTANCE_MI\"].isna() | (out[\"NEAR_DC_DISTANCE_MI\"] <= nearest_within_miles))\n    )\n    out[\"EVENT_PPG\"]    = np.where(mask_near_dc, out[\"NEAR_DC_PPG_NO_TAX\"], out[\"WEX_STATE_NET_EX_MEDIAN\"])\n    out[\"PRICE_SOURCE\"] = np.where(mask_near_dc, \"DC_NEARBY\", \"WEX_MEDIAN\")\n    return out",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1248b869-5337-4d0c-8f25-3c965fda4532",
   "metadata": {
    "language": "python",
    "name": "prepare_events_for_mileage_calc"
   },
   "outputs": [],
   "source": "def prepare_events_for_miles(session, dot_events_sp):\n    DOT_TBL    = \"DEV_DOT_MLAI.DTI_FUEL.DTI_DOT_LOCATIONS\"  # DOT_NAME, LAT, LON, GEOG\n    STATES_TBL = \"DEV_DOT_MLAI.DTI_FUEL.STATES_RAW\"          # NAME, GEOG\n\n    e = dot_events_sp.alias(\"e\")\n    d = session.table(DOT_TBL).alias(\"d\")\n    s = session.table(STATES_TBL).alias(\"s\")\n\n    # Be tolerant of either LOAD_NUMBER or LOAD_NO coming in\n    e_cols = {c.upper(): c for c in e.columns}\n    load_col     = e_cols.get(\"LOAD_NUMBER\") or e_cols.get(\"LOAD_NO\")\n    from_seq_col = e_cols.get(\"FROM_SEQ\") or \"FROM_SEQ\"\n    to_seq_col   = e_cols.get(\"TO_SEQ\")   or \"TO_SEQ\"\n    dist_col     = e_cols.get(\"DIST_MI\")  or \"DIST_MI\"\n\n    if not load_col:\n        raise ValueError(f\"dot_events_sp is missing LOAD_NO/LOAD_NUMBER. Columns: {e.columns}\")\n\n    # Join in DOT geogs and state polygons (no string-literal SQL; use API to avoid quoting surprises)\n    base = (\n        e.join(d, e[\"DOT_NAME\"] == d[\"DOT_NAME\"], how=\"left\")\n         .join(s, F.call_function(\"ST_CONTAINS\", s[\"GEOG\"], d[\"GEOG\"]), how=\"left\")\n         .with_column(\"LOAD_NO\",    e[load_col])  # normalize for downstream pandas\n         .with_column(\"LON\",       d[\"LON\"])\n         .with_column(\"LAT\",        d[\"LAT\"])\n         .with_column(\"STATE\",      s[\"NAME\"])\n         .with_column(\"EVENT_TYPE\", F.lit(\"DOT\"))\n         .with_column(\"IS_DC\",      F.lit(False))\n    )\n\n    # Window for sequencing events per load\n    w = Window.partition_by(\"LOAD_NO\").order_by(\n        F.col(from_seq_col), F.col(to_seq_col), F.col(dist_col)\n    )\n\n    out_sp = (\n        base.with_column(\"EVENT_SEQ\", F.row_number().over(w))\n            .select(\n                \"LOAD_NO\", \"EVENT_SEQ\", \"EVENT_TYPE\", \"IS_DC\",\n                \"STATE\", \"LON\", \"LAT\",\n                d[\"DOT_NAME\"].alias(\"DOT_NAME\"),\n                F.col(dist_col).alias(\"DIST_MI\"),\n                F.col(from_seq_col).alias(\"FROM_SEQ\"),\n                F.col(to_seq_col).alias(\"TO_SEQ\"),\n            )\n    )\n\n    return out_sp.to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cbe6e9eb-32db-4605-b57b-236de84803d7",
   "metadata": {
    "language": "python",
    "name": "state_key"
   },
   "outputs": [],
   "source": "def _state_key(s: str) -> str | None:\n    if not s: return None\n    s = s.strip()\n    return STATE_TO_ABBR.get(s, s).upper()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07176146-0725-4a1d-82b6-df6c5d80e0e1",
   "metadata": {
    "language": "python",
    "name": "build_state_events"
   },
   "outputs": [],
   "source": "def build_state_events_for_rcsp(state_segs_sp) -> \"pd.DataFrame\":\n    \"\"\"\n    Turn state segments into a minimal events frame for RCSP:\n      LOAD_NO, EVENT_SEQ, EVENT_TYPE='STATE', STATE, EVENT_MILES_TO_NEXT\n    EVENT_MILES_TO_NEXT is set to the segment miles (distance to the next state boundary).\n    \"\"\"\n    import pandas as pd\n\n    segs = (\n        state_segs_sp\n        .select(\"LOAD_NUMBER\",\"FROM_SEQ\",\"TO_SEQ\",\"STATE\",\"SEG_MI_SCALED\",\"START_IDX\")\n        .to_pandas()\n        .sort_values([\"LOAD_NUMBER\",\"FROM_SEQ\",\"START_IDX\"])\n        .reset_index(drop=True)\n    )\n\n    out = []\n    for load_no, grp in segs.groupby(\"LOAD_NUMBER\", sort=False):\n        seq = 1\n        for r in grp.itertuples(index=False):\n            out.append({\n                \"LOAD_NO\": int(load_no),\n                \"EVENT_SEQ\": seq,\n                \"EVENT_TYPE\": \"STATE\",\n                \"STATE\": r.STATE,\n                # RCSP uses this to compute miles-to-exit via cumulative miles\n                \"EVENT_MILES_TO_NEXT\": float(r.SEG_MI_SCALED) if r.SEG_MI_SCALED is not None else 0.0,\n                \"FROM_SEQ\": int(r.FROM_SEQ),\n                \"TO_SEQ\": int(r.TO_SEQ),\n                \"LON\": None,\n                \"LAT\": None,\n            })\n            seq += 1\n\n    return pd.DataFrame(out)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03402b70-54fd-445c-a4d1-3c9a397a3ed4",
   "metadata": {
    "language": "python",
    "name": "miles_to_exit"
   },
   "outputs": [],
   "source": "def annotate_miles_to_exit(df: \"pd.DataFrame\") -> \"pd.DataFrame\":\n    # assumes df already sorted by LOAD_NO, EVENT_SEQ\n    df = df.copy()\n    df[\"HOS_MILES_TO_EXIT\"] = None\n    for load_no, sub in df.groupby(\"LOAD_NO\", sort=False):\n        types  = sub[\"EVENT_TYPE\"].str.upper().tolist()\n        states = sub[\"STATE\"].astype(str).tolist()\n        miles  = sub[\"EVENT_MILES_TO_NEXT\"].fillna(0.0).astype(float).tolist()\n        idxs   = sub.index.tolist()\n\n        for pos, idx in enumerate(idxs):\n            if types[pos] != \"STATE\":\n                continue\n            cur = states[pos]\n            s = 0.0\n            j = pos\n            # Sum until the next STATE row with a different state (exclusive)\n            while j < len(miles):\n                s += miles[j]\n                j += 1\n                if j < len(miles) and types[j] == \"STATE\" and states[j] != cur:\n                    break\n            df.at[idx, \"HOS_MILES_TO_EXIT\"] = round(s, 3)\n    return df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85ee970f-b84f-4257-91a1-bf8703122a7c",
   "metadata": {
    "language": "python",
    "name": "annotate_rscp_fees"
   },
   "outputs": [],
   "source": "def annotate_rcsp_fee(\n    events_df,\n    load_no,\n    *,\n    default_fee: float = 100.0,\n    plan_speed_mph: float = 50.0,\n    start_drive_hours: float = 10.0,\n    free_window_hours: float = 1.0,  # NEW\n):\n    import pandas as pd\n    if events_df is None or len(events_df) == 0:\n        return events_df\n\n    df = events_df.copy()\n    df = df.sort_values([\"LOAD_NO\",\"EVENT_SEQ\"]).reset_index(drop=True)\n    df[\"EVENT_MILES_TO_NEXT\"] = pd.to_numeric(df.get(\"EVENT_MILES_TO_NEXT\", 0.0), errors=\"coerce\").fillna(0.0)\n\n    if \"HOS_MILES_TO_EXIT\" not in df.columns or df[\"HOS_MILES_TO_EXIT\"].isna().any():\n        df = annotate_miles_to_exit(df)\n\n    df[\"RCSP_PAY\"]          = False\n    df[\"RCSP_RATIONALE\"]    = \"\"\n    df[\"HOS_REM_DRIVE_H\"]   = None\n    df[\"HOS_AVG_SPEED_MPH\"] = plan_speed_mph\n    df[\"RCSP_FEE\"]          = default_fee\n    df[\"HOS_FREE_WINDOW_H\"] = float(free_window_hours)  # audit aid\n\n    t = df[\"EVENT_TYPE\"].str.upper()\n    eligible = t.eq(\"STATE\") | t.eq(\"DC\") | (t.eq(\"STOP\") & df.get(\"IS_DC\", False).fillna(False))\n    hos = float(start_drive_hours)\n\n    for i, r in df[df[\"LOAD_NO\"] == load_no].iterrows():\n        df.at[i, \"HOS_REM_DRIVE_H\"] = hos\n        free_window = hos <= float(free_window_hours)\n        if free_window:\n            df.at[i, \"RCSP_FEE\"] = 0.0\n            df.at[i, \"RCSP_PAY\"] = False\n            df.at[i, \"RCSP_RATIONALE\"] = f\"Free fuel window: remaining HOS ≤ {free_window_hours}h.\"\n        else:\n            if eligible.at[i]:\n                df.at[i, \"RCSP_FEE\"] = float(default_fee)\n                df.at[i, \"RCSP_PAY\"] = True\n                if t.at[i] == \"STATE\":\n                    miles_to_exit = df.at[i, \"HOS_MILES_TO_EXIT\"]\n                    df.at[i, \"RCSP_RATIONALE\"] = f\"State boundary decision; exit≈{miles_to_exit if miles_to_exit is not None else 'unknown'} mi.\"\n                else:\n                    df.at[i, \"RCSP_RATIONALE\"] = \"Fuel-eligible DC/STOP.\"\n\n        leg_mi = float(r[\"EVENT_MILES_TO_NEXT\"] or 0.0)\n        hos = max(0.0, hos - (leg_mi / plan_speed_mph))\n        if hos <= 0.0:\n            hos = 11.0  # reset\n    return df\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a38a3a0-571b-4bd1-90fc-f84651dc693f",
   "metadata": {
    "language": "python",
    "name": "unify_events"
   },
   "outputs": [],
   "source": "def build_unified_events(session, *, stops_sp, legs_sp, dc_events_pd, boundary_events_pd) -> \"pd.DataFrame\":\n    # Build leg miles and cumulative stop mile-markers (unchanged)\n    legs_pd = (\n        legs_sp.select(\"LOAD_NUMBER\", \"FROM_SEQ\", \"TO_SEQ\", \"MILES_PCMILER\")\n               .to_pandas()\n               .sort_values([\"LOAD_NUMBER\",\"FROM_SEQ\"])\n               .reset_index(drop=True)\n    )\n    legs_pd[\"LEG_MI\"] = pd.to_numeric(legs_pd[\"MILES_PCMILER\"], errors=\"coerce\").fillna(0.0)\n\n    mm_at_stop = {}\n    for load_no, g in legs_pd.groupby(\"LOAD_NUMBER\", sort=False):\n        g = g.sort_values(\"FROM_SEQ\")\n        mm = {0: 0.0}\n        for r in g.itertuples(index=False):\n            prev = int(r.FROM_SEQ); nxt = int(r.TO_SEQ)\n            base = mm.get(prev, 0.0)\n            mm[nxt] = base + float(r.LEG_MI or 0.0)\n        mm_at_stop[int(load_no)] = mm\n\n    st_pd = (\n        stops_sp.select(\"LOAD_NUMBER\", \"LOAD_STOP_SEQUENCE_NO\", \"STATE\", \"LON\", \"LAT\", \"DOT_DOMICILEABBREVIATION\")\n                .to_pandas()\n                .sort_values([\"LOAD_NUMBER\", \"LOAD_STOP_SEQUENCE_NO\"])\n                .reset_index(drop=True)\n    )\n\n    def _mm_for_stop(row):\n        m = mm_at_stop.get(int(row[\"LOAD_NUMBER\"]), {})\n        return float(m.get(int(row[\"LOAD_STOP_SEQUENCE_NO\"]), 0.0))\n\n    st_pd[\"MM_APPROX_MI\"] = st_pd.apply(_mm_for_stop, axis=1)\n\n    stop_ev = st_pd.rename(columns={\"LOAD_NUMBER\": \"LOAD_NO\"})\n    stop_ev[\"FROM_SEQ\"]   = stop_ev[\"LOAD_STOP_SEQUENCE_NO\"] - 1\n    stop_ev[\"TO_SEQ\"]     = stop_ev[\"LOAD_STOP_SEQUENCE_NO\"]\n    stop_ev[\"EVENT_TYPE\"] = \"STOP\"\n    stop_ev[\"IS_DC\"]      = stop_ev[\"DOT_DOMICILEABBREVIATION\"].fillna(\"\").astype(str).str.len().gt(0)\n    stop_ev[\"IS_STATE_BOUNDARY\"] = False\n    stop_ev[\"DOT_NAME\"] = None\n    stop_ev = stop_ev[[\n        \"LOAD_NO\",\"FROM_SEQ\",\"TO_SEQ\",\"STATE\",\"LON\",\"LAT\",\n        \"MM_APPROX_MI\",\"EVENT_TYPE\",\"IS_DC\",\"IS_STATE_BOUNDARY\",\"DOT_NAME\"\n    ]]\n\n    # Origin STATE event\n    origin_state_df = (\n        st_pd.sort_values([\"LOAD_NUMBER\", \"LOAD_STOP_SEQUENCE_NO\"])\n             .groupby(\"LOAD_NUMBER\", as_index=False).head(1)\n             .assign(\n                 FROM_SEQ=-1, TO_SEQ=0,\n                 MM_APPROX_MI=0.0,\n                 EVENT_TYPE=\"STATE\",\n                 IS_STATE_BOUNDARY=False,\n                 IS_DC=False\n             )\n             .rename(columns={\"LOAD_NUMBER\": \"LOAD_NO\"})\n             [[\"LOAD_NO\", \"FROM_SEQ\", \"TO_SEQ\", \"STATE\", \"LON\", \"LAT\",\n               \"MM_APPROX_MI\", \"EVENT_TYPE\", \"IS_DC\", \"IS_STATE_BOUNDARY\"]]\n    )\n\n    # DC events (per-leg mm) -> globalize by adding leg base offset\n    dc_ev = dc_events_pd.copy()\n    dc_ev[\"IS_STATE_BOUNDARY\"] = False\n    if not dc_ev.empty:\n        dc_ev[\"LEG_BASE_MM\"] = dc_ev.apply(lambda r: mm_at_stop.get(int(r[\"LOAD_NO\"]), {}).get(int(r[\"FROM_SEQ\"]), 0.0), axis=1)\n        dc_ev[\"MM_APPROX_MI\"] = pd.to_numeric(dc_ev[\"MM_APPROX_MI\"], errors=\"coerce\").fillna(0.0) + dc_ev[\"LEG_BASE_MM\"]\n        dc_ev = dc_ev.drop(columns=[\"LEG_BASE_MM\"])\n    dc_ev = dc_ev[[\"LOAD_NO\", \"FROM_SEQ\", \"TO_SEQ\", \"STATE\", \"LON\", \"LAT\",\n                   \"MM_APPROX_MI\", \"EVENT_TYPE\", \"IS_DC\", \"IS_STATE_BOUNDARY\", \"DOT_NAME\"]]\n\n    # State boundary events (per-leg mm) -> globalize by adding leg base offset\n    b_ev = boundary_events_pd.copy()\n    b_ev[\"IS_DC\"] = False\n    b_ev[\"DOT_NAME\"] = None\n    if not b_ev.empty:\n        b_ev[\"LEG_BASE_MM\"] = b_ev.apply(lambda r: mm_at_stop.get(int(r[\"LOAD_NO\"]), {}).get(int(r[\"FROM_SEQ\"]), 0.0), axis=1)\n        b_ev[\"MM_APPROX_MI\"] = pd.to_numeric(b_ev[\"MM_APPROX_MI\"], errors=\"coerce\").fillna(0.0) + b_ev[\"LEG_BASE_MM\"]\n        b_ev = b_ev.drop(columns=[\"LEG_BASE_MM\"])\n    b_ev = b_ev[[\"LOAD_NO\", \"FROM_SEQ\", \"TO_SEQ\", \"STATE\", \"LON\", \"LAT\",\n                 \"MM_APPROX_MI\", \"EVENT_TYPE\", \"IS_DC\", \"IS_STATE_BOUNDARY\", \"DOT_NAME\"]]\n\n    ev = pd.concat([stop_ev, origin_state_df.assign(DOT_NAME=None), b_ev, dc_ev], ignore_index=True)\n\n    # Normalize states to two-letter codes\n    ev[\"STATE\"] = ev[\"STATE\"].apply(lambda s: _state_key(s) or s)\n\n    # Dedup/origin ordering (as you had)\n    mask_stop_is_dc0 = ev[\"EVENT_TYPE\"].eq(\"STOP\") & ev[\"IS_DC\"].fillna(False) & ev[\"MM_APPROX_MI\"].abs().le(1e-6)\n    ev.loc[mask_stop_is_dc0, \"EVENT_TYPE\"] = \"DC\"\n\n    mm0 = ev[\"MM_APPROX_MI\"].abs().le(1e-6)\n    dup_key = (\n        ev[\"LOAD_NO\"].astype(str) + \"|\" +\n        ev[\"EVENT_TYPE\"].astype(str) + \"|\" +\n        ev[\"LON\"].round(6).astype(str) + \"|\" +\n        ev[\"LAT\"].round(6).astype(str) + \"|\" +\n        ev[\"MM_APPROX_MI\"].round(3).astype(str)\n    )\n    ev = ev.loc[~(mm0 & dup_key.duplicated(keep=\"first\"))].copy()\n\n    rank = {\"STOP\": 0, \"DC\": 0, \"STATE\": 1}\n    ev[\"__r\"] = ev[\"EVENT_TYPE\"].str.upper().map(rank).fillna(9)\n    ev = ev.sort_values([\"LOAD_NO\", \"MM_APPROX_MI\", \"__r\"]).drop(columns=\"__r\").reset_index(drop=True)\n\n    ev[\"EVENT_SEQ\"] = ev.groupby(\"LOAD_NO\", sort=False).cumcount() + 1\n    return ev\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c60d485-c7b5-49b1-a097-cc2cfcd229b7",
   "metadata": {
    "language": "python",
    "name": "rcsp_optimizer"
   },
   "outputs": [],
   "source": "def optimize_fuel_plan_rcsp(\n        events,                # list[dict] in route order\n        initial_fuel,          # gallons on board at first event\n        mpg,                   # truck MPG\n        tank_capacity,         # max gallons\n        safety_buffer,         # reserve gallons that must remain after each leg\n        delta: float = 0.1,    # purchase “click” size in gallons\n        stop_fee: float = 100.0,  # fee if a purchase happens and RCSP_FEE absent\n):\n    \"\"\"\n    Label-setting RCSP on a 3-D state space:\n      state = (i, u, paid) where\n        i     : event index\n        u     : fuel units on board (integerised gallons / delta)\n        paid  : 0 if stop fee not yet paid at node i, 1 if already paid at i\n\n    Returns: list of (EVENT_SEQ, gallons_bought) tuples aggregated per stop.\n    \"\"\"\n    cons_per_mile = 1.0 / float(mpg)\n    N = len(events)\n    max_units = int(float(tank_capacity) / delta)\n    buf_units = int(float(safety_buffer) / delta)\n\n    # miles needed (in fuel units) to reach i+1\n    leg_req = [\n        int(math.ceil(((ev.get(\"EVENT_MILES_TO_NEXT\") or 0.0) * cons_per_mile) / delta))\n        for ev in events\n    ]\n\n    INF = float(\"inf\")\n    # dist[paid][i][u] = min cost to be at (i,u,paid)\n    dist = [[[INF]*(max_units + 1) for _ in range(N)] for _ in range(2)]\n    prev = [[[None]*(max_units + 1) for _ in range(N)] for _ in range(2)]\n\n    start_u = min(int(float(initial_fuel) / delta), max_units)\n    dist[1][0][start_u] = 0.0                 # at origin; treat fee as already paid\n    heap = [(0.0, 0, start_u, 1)]             # (cost, i, u, paid)\n\n    while heap:\n        cost, i, u, paid = heapq.heappop(heap)\n        if cost > dist[paid][i][u]:\n            continue\n\n        # Drive to next node\n        if i + 1 < N:\n            req = leg_req[i]\n            if req == 0:\n                # Zero-mile hop: allow advancing without enforcing the buffer\n                v = u\n                if cost < dist[0][i+1][v]:\n                    dist[0][i+1][v] = cost\n                    prev[0][i+1][v] = (paid, i, u, 0.0)\n                    heapq.heappush(heap, (cost, i+1, v, 0))\n            elif u - req >= buf_units:\n                # Real driving leg: enforce reserve\n                v = u - req\n                if cost < dist[0][i+1][v]:\n                    dist[0][i+1][v] = cost\n                    prev[0][i+1][v] = (paid, i, u, 0.0)\n                    heapq.heappush(heap, (cost, i+1, v, 0))\n\n        # Buy one delta gallon at node i\n        price = events[i].get(\"PRICE\")\n        if price is not None and u < max_units:\n            v = u + 1\n            node_fee = events[i].get(\"RCSP_FEE\", stop_fee)\n            fee_inc  = 0.0 if paid else float(node_fee)\n            new_c    = cost + fee_inc + float(price) * delta\n            if new_c < dist[1][i][v]:\n                dist[1][i][v] = new_c\n                prev[1][i][v] = (paid, i, u, delta)\n                heapq.heappush(heap, (new_c, i, v, 1))\n\n    # Best terminal state\n    end_i = N - 1\n    best_u, best_paid, best_c = None, None, INF\n    for p in (0, 1):\n        for u in range(buf_units, max_units + 1):\n            if dist[p][end_i][u] < best_c:\n                best_c, best_u, best_paid = dist[p][end_i][u], u, p\n    if best_u is None:\n        raise RuntimeError(\"No feasible refuelling plan found.\")\n\n    # Backtrack purchases → aggregate by EVENT_SEQ\n    plan = []\n    cur = (best_paid, end_i, best_u)\n    while True:\n        p, i, u = cur\n        prv = prev[p][i][u]\n        if prv is None:\n            break\n        pp, pi, pu, bought = prv\n        if bought > 0:\n            plan.append((events[i][\"EVENT_SEQ\"], bought))\n        cur = (pp, pi, pu)\n\n    plan.reverse()\n    agg = {}\n    for seq, g in plan:\n        agg[int(seq)] = round(agg.get(int(seq), 0.0) + float(g), 1)\n    return [(seq, gals) for seq, gals in agg.items()]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31ad5119-dbf4-40f9-a252-117ec7018aba",
   "metadata": {
    "language": "python",
    "name": "build_state_boundry_events_using_geotunnel"
   },
   "outputs": [],
   "source": "def build_state_boundary_events_from_geotunnel(routes_with_geotunnel_pd) -> \"pd.DataFrame\":\n    import pandas as pd\n    rows = []\n    for r in routes_with_geotunnel_pd.itertuples(index=False):\n        load_no = getattr(r, \"LOAD_NO\")\n        from_seq = getattr(r, \"LOAD_STOP_SEQUENCE_NO\")\n        to_seq = from_seq + 1\n        for t in getattr(r, \"STATE_TRANSITIONS\", None) or []:\n            rows.append({\n                \"LOAD_NO\": int(load_no),\n                \"FROM_SEQ\": int(from_seq),\n                \"TO_SEQ\": int(to_seq),\n                \"STATE\": t.get(\"to_state\"),\n                \"LON\": float(t.get(\"transition_lon\")) if t.get(\"transition_lon\") is not None else None,\n                \"LAT\": float(t.get(\"transition_lat\")) if t.get(\"transition_lat\") is not None else None,\n                \"MM_APPROX_MI\": float(t.get(\"mm\")) if t.get(\"mm\") is not None else None,\n                \"EVENT_TYPE\": \"STATE\",\n                \"IS_STATE_BOUNDARY\": True,\n            })\n    return pd.DataFrame(rows, columns=[\n        \"LOAD_NO\",\"FROM_SEQ\",\"TO_SEQ\",\"STATE\",\"LON\",\"LAT\",\"MM_APPROX_MI\",\n        \"EVENT_TYPE\",\"IS_STATE_BOUNDARY\"\n    ])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd829a0b-3dea-4ca9-8c45-8f32832dad84",
   "metadata": {
    "language": "python",
    "name": "polyline_length"
   },
   "outputs": [],
   "source": "def _polyline_length_mi(points):\n    \"\"\"Accurate haversine length of a polyline [(lon,lat), ...] in miles.\"\"\"\n    if not points: \n        return 0.0\n    from math import radians, sin, cos, asin, sqrt\n    R = 3958.7613\n    total = 0.0\n    lon0, lat0 = points[0]\n    for lon1, lat1 in points[1:]:\n        dlat = radians(lat1 - lat0); dlon = radians(lon1 - lon0)\n        a = sin(dlat/2)**2 + cos(radians(lat0))*cos(radians(lat1))*sin(dlon/2)**2\n        total += 2 * R * asin(sqrt(a))\n        lon0, lat0 = lon1, lat1\n    return total\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "697d7821-af62-4f17-8a59-34a5c98ed988",
   "metadata": {
    "language": "python",
    "name": "format_plan"
   },
   "outputs": [],
   "source": "def _format_plan_text(events_df, plan_list, params):\n    rows = []\n    total_cost = 0.0\n    by_seq = events_df.set_index(\"EVENT_SEQ\")\n    for seq, gals in plan_list:\n        r = by_seq.loc[int(seq)]\n        et = str(r.get(\"EVENT_TYPE\") or \"\").upper()\n        is_dc = bool(r.get(\"IS_DC\"))\n        if et in (\"DC\", \"STOP\") and is_dc:\n            name = r.get(\"DOT_NAME\")\n            if not name:\n                near_city  = r.get(\"NEAR_DC_CITY\")\n                near_state = r.get(\"NEAR_DC_STATE\") or r.get(\"STATE\")\n                name = f\"{near_city}, {near_state}\" if near_city else str(near_state or \"DC\")\n            loc = f\"DC {name}\"\n        elif et == \"STATE\":\n            loc = f\"State line: {r.get('STATE')}\"\n        else:\n            loc = f\"Stop #{int(r.get('TO_SEQ') or r.get('FROM_SEQ') or 0)}\"\n\n        ppg  = float(r.get(\"PRICE\") or 0.0)\n        cost = round(gals * ppg, 2) if ppg else None\n        if cost is not None:\n            total_cost += cost\n        rows.append(f\"- Buy **{gals:.1f} gal** at {loc}\" + (f\" @ ${ppg:.3f}/gal (≈${cost:.2f})\" if ppg else \"\"))\n\n    header = (f\"Load {int(params['load'])}: Start {params['initial']} gal in {int(params['tank'])} gal tank \"\n              f\"(MPG {params['mpg']}, reserve {params['buffer']} gal).\")\n    footer = f\"Estimated fuel spend: ${total_cost:.2f}\" if total_cost else \"\"\n    return \"\\n\".join([header, \"\", *rows, \"\", footer]).strip()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc2712ba-9f5d-4b64-a9fb-3c0800433228",
   "metadata": {
    "language": "python",
    "name": "LLM_Output"
   },
   "outputs": [],
   "source": "def get_llm_plan(events_all, plan_list, params: dict) -> str:\n    \"\"\"\n    Use GPT-5-mini to render a clean, readable Markdown fuel plan.\n    Falls back to a simple template if the API fails.\n    \"\"\"\n    try:\n        buy_map = {int(seq): float(gal) for (seq, gal) in plan_list}\n        view_cols = [\n            \"EVENT_SEQ\",\"EVENT_TYPE\",\"LOCATION_TYPE\",\"STATE\",\"DOT_NAME\",\n            \"NEAR_DC_CITY\",\"NEAR_DC_STATE\",\"EVENT_PPG\",\"PRICE\",\n            \"EVENT_MILES_TO_NEXT\",\"GALLONS_TO_NEXT\"\n        ]\n        df = events_all.copy()\n        for c in view_cols:\n            if c not in df.columns:\n                df[c] = np.nan\n        df[\"BUY_GALLONS\"] = df[\"EVENT_SEQ\"].map(buy_map).fillna(0.0)\n\n        payload = {\n            \"meta\": {\n                \"load_number\": params.get(\"load\"),\n                \"initial_fuel_gal\": float(params.get(\"initial\", 0)),\n                \"tank_capacity_gal\": float(params.get(\"tank\", 0)),\n                \"mpg\": float(params.get(\"mpg\", 0)),\n                \"reserve_gal\": float(params.get(\"buffer\", 0)),\n            },\n            \"events\": df[view_cols + [\"BUY_GALLONS\"]].replace({np.nan: None}).to_dict(\"records\"),\n        }\n\n        system = (\n            \"You format trucking fuel plans. Output concise Markdown only. \"\n            \"Use the JSON exactly—no invented values. Include: title; quick summary; \"\n            \"itemized buys (location/state, event_seq, price, gallons, est cost); \"\n            \"compact table of all hops (seq, type, loc/state, mi_to_next, gal_to_next, price); \"\n            \"and totals (gallons bought, spend). Money to 2 decimals.\"\n        )\n        user = f\"Render this fuel plan nicely. JSON:\\n\\n{payload}\"\n\n        response = openai.chat.completions.create(\n            model=OPENAI_MODEL,\n            max_completion_tokens=MAX_TOKENS,\n            messages=[\n                {\"role\": \"system\", \"content\": system},\n                {\"role\": \"user\", \"content\": user},\n            ],\n        )\n        return response.choices[0].message.content.strip()\n\n    except Exception:\n        buys = [f\"- Seq {int(seq)}: buy **{float(g):.1f} gal**\" for (seq, g) in plan_list]\n        return (\n            f\"Load {params.get('load')}: start {params.get('initial')} gal in {params.get('tank')} gal tank \"\n            f\"(MPG {params.get('mpg')}, reserve {params.get('buffer')} gal).\\n\\n\" + \"\\n\".join(buys)\n        )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "670a58fd-b3eb-4a67-b59f-8f9f9c9c4770",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "def render_compact_fuel_plan(events_df, plan_rows, title_prefix=None):\n    \"\"\"\n    events_df: DataFrame with EVENT_SEQ, EVENT_TYPE, STATE, DOT_NAME, PRICE, EVENT_MILES_TO_NEXT\n               (optional: NEAR_DC_CITY, NEAR_DC_STATE, CITY)\n    plan_rows: list of dicts like {'EVENT_SEQ': int, 'BUY_GALLONS': float}\n    \"\"\"\n    import math\n    import numpy as np\n    import pandas as pd\n\n    header = \"Fuel Plan\" if not title_prefix else f\"{title_prefix}\\n\\nFuel Plan\"\n    if events_df is None or len(events_df) == 0:\n        return f\"No fuel purchases.\\n\\n{header}\\n\"\n\n    # Prepare and compute fallback leg miles (use next event's miles if current is 0)\n    df = events_df.copy().sort_values(\"EVENT_SEQ\").reset_index(drop=True)\n    df[\"EVENT_MILES_TO_NEXT\"] = pd.to_numeric(df.get(\"EVENT_MILES_TO_NEXT\"), errors=\"coerce\").fillna(0.0)\n    miles = df[\"EVENT_MILES_TO_NEXT\"].to_numpy()\n    miles_next = np.r_[miles[1:], 0.0]\n    df[\"LEG_MILES_FOR_LINE\"] = np.where(miles > 0, miles, miles_next)\n    leg_mi_map = dict(zip(df[\"EVENT_SEQ\"].astype(int), df[\"LEG_MILES_FOR_LINE\"]))\n\n    by_seq = df.set_index(\"EVENT_SEQ\", drop=False)\n\n    def _uc(x):  # uppercase safe\n        return str(x or \"\").strip().upper()\n\n    def label_for(seq: int) -> str:\n        r = by_seq.loc[int(seq)]\n        et = _uc(r.get(\"EVENT_TYPE\"))\n        state_abbr = _uc(r.get(\"STATE\"))\n        dot = (r.get(\"DOT_NAME\") or \"\").strip()\n\n        if et == \"DC\":\n            city = (r.get(\"NEAR_DC_CITY\") or (dot.split(\",\")[0].strip() if dot else \"\")).title()\n            st = _uc(r.get(\"NEAR_DC_STATE\") or state_abbr)\n            name = \", \".join([p for p in [city, st] if p])\n            return f\"{name} DC\" if name else \"DC\"\n\n        if et == \"STATE\":\n            prev_state = _uc(by_seq.loc[int(seq) - 1][\"STATE\"]) if (int(seq) - 1) in by_seq.index else \"\"\n            if prev_state and prev_state != state_abbr:\n                return f\"{state_abbr} (entering from {prev_state})\"\n            return state_abbr or \"STATE\"\n\n        # Default: try City, ST; otherwise just ST\n        city = (r.get(\"CITY\") or \"\").title()\n        return (f\"{city}, {state_abbr}\".strip(\", \") if city else state_abbr) or \"Location\"\n\n    items, lines = [], []\n\n    for i, row in enumerate(plan_rows, start=1):\n        seq = int(row[\"EVENT_SEQ\"])\n        buy = float(row[\"BUY_GALLONS\"])\n        r = by_seq.loc[seq]\n    \n        lab = label_for(seq)\n        leg_mi = int(round(float(leg_mi_map.get(seq, 0.0))))\n        items.append(f\"buy {buy:.1f} gallons at {lab}\")\n    \n        # robust price handling (no $0.000 when missing)\n        raw_price = r.get(\"PRICE\")\n        try:\n            price = float(raw_price)\n            if math.isnan(price):\n                price = None\n        except (TypeError, ValueError):\n            price = None\n    \n        price_str = f\"${price:.3f}/gal\" if price is not None else \"N/A\"\n        cost_str  = f\"${buy*price:.0f}\" if price is not None else \"N/A\"\n    \n        lines.append(\n            f\"{i}. {lab}: buy {buy:.1f} gal @ {price_str} = {cost_str}\"\n            + (f\"; next leg {leg_mi} mi\" if leg_mi > 0 else \"\")\n        )\n\n    # summary sentence\n    if items:\n        if len(items) > 1:\n            summary = \", then \".join(items[:-1]) + f\", and lastly {items[-1]}.\"\n        else:\n            summary = items[0][:1].upper() + items[0][1:] + \".\"\n    else:\n        summary = \"No fuel purchases.\"\n\n\n    return f\"{summary}\\n\\n{header}\\n\" + \"\\n\".join(lines)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "240c2aa5-406e-4e06-bdd9-00d45a45f855",
   "metadata": {
    "language": "python",
    "name": "main_entry_point"
   },
   "outputs": [],
   "source": "def main(session: Session, inputs) -> dict:\n    run_id = getattr(inputs, \"run_id\", uuid.uuid4().hex)\n    stops = inputs.stops_df\n    legs_sp = build_legs_and_compute_mileage(session=session, stops_df=stops, load_number=inputs.load_number)\n\n    routes_with_geotunnel = build_routes_df_for_events(session, stops, legs_sp, pcmiler_api_key=PC_MILER_API_KEY)\n    boundary_events_pd = build_state_boundary_events_from_geotunnel(routes_with_geotunnel)\n    dc_events_pd = expand_dc_candidates_from_geotunnel(\n        session,\n        routes_with_geotunnel,\n        radius_mi=getattr(inputs, \"dc_radius_mi\", 5.0),\n        step_mi=getattr(inputs, \"geotunnel_step_mi\", 1.0),\n    )\n\n    events_all = build_unified_events(\n        session,\n        stops_sp=stops,\n        legs_sp=legs_sp,\n        dc_events_pd=dc_events_pd,\n        boundary_events_pd=boundary_events_pd,\n    ).reset_index(drop=True)\n\n    mileage_fn = create_mileage_cache(PC_MILER_API_KEY)\n    events_all = add_event_miles(events_all, mileage_fn)\n\n    mm_next = events_all[\"MM_APPROX_MI\"].groupby(events_all[\"LOAD_NO\"]).shift(-1)\n    hop_mm  = (mm_next - events_all[\"MM_APPROX_MI\"]).clip(lower=0).fillna(0.0)\n    events_all[\"EVENT_MILES_TO_NEXT\"] = np.where(\n        events_all[\"EVENT_MILES_TO_NEXT\"].notna() & (events_all[\"EVENT_MILES_TO_NEXT\"] > 0),\n        events_all[\"EVENT_MILES_TO_NEXT\"],\n        hop_mm,\n    )\n    events_all[\"EVENT_MILES_TO_NEXT\"] = (\n        pd.to_numeric(events_all[\"EVENT_MILES_TO_NEXT\"], errors=\"coerce\").fillna(0.0).clip(lower=0.0)\n    )\n\n    # NEW: harmonize Σ hops with final mile-marker\n    events_all = reconcile_event_miles(events_all, tol_mi=0.5)\n\n    events_all = annotate_miles_to_exit(events_all)\n\n    dc_fuel_df, state_price = build_price_inputs(session)\n    events_all = append_price_info(events_all, dc_fuel_df, state_price)\n\n    is_state = events_all[\"EVENT_TYPE\"].str.upper().eq(\"STATE\")\n    is_dc    = events_all[\"EVENT_TYPE\"].str.upper().eq(\"DC\") | (\n                 events_all[\"EVENT_TYPE\"].str.upper().eq(\"STOP\") & events_all[\"IS_DC\"].fillna(False)\n               )\n    eligible = (is_state | is_dc)\n    events_all[\"PRICE\"] = np.where(eligible, events_all[\"EVENT_PPG\"], np.nan)\n    \n    # Origin feasibility override (pre-trip top-off)\n    events_all = events_all.sort_values([\"LOAD_NO\",\"EVENT_SEQ\"]).reset_index(drop=True)\n    # Find the first hop that actually consumes miles\n    next_pos = events_all[events_all[\"EVENT_MILES_TO_NEXT\"] > 0].head(1)\n    origin_needs_buy = False\n    if not next_pos.empty:\n        need_gal_first_move = float(next_pos.iloc[0][\"EVENT_MILES_TO_NEXT\"]) / float(inputs.mpg)\n        origin_needs_buy = (float(inputs.initial_fuel) - need_gal_first_move) < float(inputs.safety_buffer)\n    \n    # If we *do* need to buy before that first consuming hop, make the origin eligible/priced\n    if origin_needs_buy:\n        # ensure the very first event has a usable price (prefer nearby DC within 5 mi)\n        first = events_all.iloc[0]\n        dc_price = first.get(\"NEAR_DC_PPG_NO_TAX\")\n        dist     = first.get(\"NEAR_DC_DISTANCE_MI\")\n        chosen   = dc_price if (pd.notna(dc_price) and (pd.isna(dist) or float(dist) <= 5.0)) else first.get(\"EVENT_PPG\")\n        events_all.at[0, \"PRICE\"] = chosen\n        events_all.at[0, \"IS_DC\"] = True\n\n\n    if \"PRICE\" not in events_all.columns:\n        events_all[\"PRICE\"] = events_all[\"EVENT_PPG\"]\n\n    multiple_drivers = is_team_driving(stops)\n    \n    if multiple_drivers:\n        # HARD BYPASS: don't call annotate_rcsp_fee at all\n        events_all[\"HOS_APPLIED\"]       = False\n        events_all[\"HOS_REM_DRIVE_H\"]   = np.nan\n        events_all[\"HOS_AVG_SPEED_MPH\"] = np.nan\n        events_all[\"HOS_FREE_WINDOW_H\"] = 0.0\n        events_all[\"HOS_MILES_TO_EXIT\"] = np.nan\n        # keep fee model simple when team driving\n        events_all[\"RCSP_FEE\"]          = 100.0\n        events_all[\"RCSP_PAY\"]          = True\n    else:\n        start_drive_h = get_start_drive_hours_for_load(session, stops)\n        events_all = annotate_rcsp_fee(\n        events_all,\n        load_no=inputs.load_number,\n        default_fee=100.0,\n        plan_speed_mph=50.0,\n        start_drive_hours=float(start_drive_h),\n        free_window_hours=1.0,\n        )\n        events_all[\"HOS_APPLIED\"] = True\n\n    # ---- fee-free if HOS hits 0 inside the state segment ----\n    ev = events_all.copy()\n    \n    is_state = ev[\"EVENT_TYPE\"].str.upper().eq(\"STATE\")\n    ev[\"SEGMENT_ID\"] = is_state.cumsum()\n    \n    mph    = ev[\"HOS_AVG_SPEED_MPH\"].fillna(50.0).astype(float)\n    hos_h = pd.to_numeric(ev[\"HOS_REM_DRIVE_H\"], errors=\"coerce\").fillna(0.0)\n    seg_mi = pd.to_numeric(\n        ev[\"HOS_MILES_TO_EXIT\"].where(is_state).fillna(ev[\"EVENT_MILES_TO_NEXT\"]),\n        errors=\"coerce\"\n    ).fillna(0.0)\n    \n    hits_zero_at_state = (mph * hos_h) <= seg_mi\n    ev[\"SEG_FEE_FREE\"] = False\n    ev.loc[is_state & hits_zero_at_state, \"SEG_FEE_FREE\"] = True\n    ev[\"SEG_FEE_FREE\"] = ev.groupby(\"SEGMENT_ID\")[\"SEG_FEE_FREE\"].transform(\"max\")\n    \n    fee_eligible = ev[\"EVENT_TYPE\"].str.upper().isin([\"STATE\",\"DC\",\"STOP\"])\n    ev.loc[fee_eligible & ev[\"SEG_FEE_FREE\"], \"RCSP_FEE\"] = 0.0\n    ev.loc[fee_eligible & ev[\"SEG_FEE_FREE\"], \"RCSP_PAY\"] = False\n    ev.loc[fee_eligible & ev[\"SEG_FEE_FREE\"], \"RCSP_RATIONALE\"] = (\n        ev.loc[fee_eligible & ev[\"SEG_FEE_FREE\"], \"RCSP_RATIONALE\"].fillna(\"\").str.rstrip()\n        + \" HOS hits 0 in this state segment; fee waived.\"\n    )\n    \n    events_all = ev\n\n\n    # Optimizer inputs\n    truck_mpg = float(inputs.mpg)\n    events_all[\"LOCATION_TYPE\"] = np.select(\n        [\n            (events_all[\"EVENT_TYPE\"].str.upper() == \"STOP\") & events_all[\"IS_DC\"].fillna(False),\n            (events_all[\"EVENT_TYPE\"].str.upper() == \"STOP\") & (~events_all[\"IS_DC\"].fillna(False)),\n            (events_all[\"EVENT_TYPE\"].str.upper() == \"STATE\"),\n            (events_all[\"EVENT_TYPE\"].str.upper() == \"DC\"),\n        ],\n        [\"DC\", \"STOP\", \"STATE\", \"DC\"],\n        default=\"SKIP\"\n    )\n    events_all[\"GALLONS_TO_NEXT\"] = (events_all[\"EVENT_MILES_TO_NEXT\"] / truck_mpg).round(2)\n\n    plan_list = optimize_fuel_plan_rcsp(\n        events_all.to_dict(orient=\"records\"),\n        initial_fuel=float(inputs.initial_fuel),\n        mpg=truck_mpg,\n        tank_capacity=float(inputs.tank_capacity),\n        safety_buffer=float(inputs.safety_buffer),\n        stop_fee=100.0,\n    )\n\n    # Previews/telemetry (optional)\n    geotunnel_preview = routes_with_geotunnel.head(50).to_dict(orient=\"records\")\n    dc_events_preview = dc_events_pd.head(100).to_dict(orient=\"records\")\n    boundary_preview  = boundary_events_pd.head(100).to_dict(orient=\"records\")\n    combined_preview  = events_all.head(50).to_dict(orient=\"records\")\n    plan_preview      = [{\"EVENT_SEQ\": int(seq), \"BUY_GALLONS\": float(g)} for (seq, g) in plan_list]\n\n    # fuel_plan_text = _format_plan_text(\n    # events_all,\n    # plan_list,\n    # params={\"load\": inputs.load_number, \"initial\": inputs.initial_fuel,\n    #             \"tank\": inputs.tank_capacity, \"mpg\": inputs.mpg, \"buffer\": inputs.safety_buffer}\n    # )\n\n    # fuel_plan_text = get_llm_plan(\n    #     events_all,\n    #     plan_list,\n    #     params={\"load\": inputs.load_number, \"initial\": inputs.initial_fuel,\n    #             \"tank\": inputs.tank_capacity, \"mpg\": inputs.mpg, \"buffer\": inputs.safety_buffer}\n    # )\n\n    events_df = events_all.reset_index(drop=True)          # the unified events you just built\n    fuel_plan_text = render_compact_fuel_plan(events_df,   # pass the events\n                                              plan_preview) \n\n    hos_cols = [c for c in events_df.columns if c.upper().startswith(\"HOS_\")]\n    safe_events_preview = (\n        events_df.drop(columns=[c for c in hos_cols if c in events_df.columns], errors=\"ignore\")\n                 .head(200)\n                 .to_dict(orient=\"records\")\n    )\n\n    # --- JSON blobs ---\n    payload_json = json.dumps(getattr(inputs, \"raw_payload\", {}))\n    fuel_params_json = json.dumps({\n        \"initial_fuel\": inputs.initial_fuel,\n        \"tank_capacity\": inputs.tank_capacity,\n        \"mpg\": inputs.mpg,\n        \"safety_buffer\": inputs.safety_buffer,\n    })\n    plan_json   = json.dumps(plan_preview)\n    events_json = json.dumps(safe_events_preview)\n    dc_json     = json.dumps(dc_events_preview)\n    state_json = boundary_events_pd.head(100).to_json(orient=\"records\")\n    \n    # --- header row -> FUEL_RUN_LOG (10 cols incl CREATED_AT) ---\n    tbl = \"DEV_DOT_MLAI.DTI_FUEL.FUEL_RUN_LOG\"\n    \n    vals = session.create_dataframe(\n        [(run_id, str(inputs.load_number), payload_json, fuel_params_json, plan_json,\n          fuel_plan_text, events_json, dc_json, state_json)],\n        schema=[\n            \"RUN_ID\",\"LOAD_NUMBER\",\"REQUEST_PAYLOAD\",\"FUEL_PARAMS\",\"FUEL_PLAN\",\n            \"FUEL_PLAN_TEXT\",\"EVENTS_PREVIEW\",\"DC_EVENTS_PREVIEW\",\"STATE_BOUNDARY_PREVIEW\",\n        ],\n    )\n    \n    tgt = session.table(tbl)\n    provided = {\n        \"RUN_ID\": F.col(\"RUN_ID\"),\n        \"LOAD_NUMBER\": F.col(\"LOAD_NUMBER\"),\n        \"REQUEST_PAYLOAD\": F.parse_json(F.col(\"REQUEST_PAYLOAD\")),\n        \"FUEL_PARAMS\": F.parse_json(F.col(\"FUEL_PARAMS\")),\n        \"FUEL_PLAN\": F.parse_json(F.col(\"FUEL_PLAN\")),\n        \"FUEL_PLAN_TEXT\": F.col(\"FUEL_PLAN_TEXT\"),\n        \"EVENTS_PREVIEW\": F.parse_json(F.col(\"EVENTS_PREVIEW\")),\n        \"DC_EVENTS_PREVIEW\": F.parse_json(F.col(\"DC_EVENTS_PREVIEW\")),\n        \"STATE_BOUNDARY_PREVIEW\": F.parse_json(F.col(\"STATE_BOUNDARY_PREVIEW\")),\n    }\n    \n    select_list = []\n    for f in tgt.schema.fields:\n        n = f.name.upper()\n        if n == \"CREATED_AT\":\n            expr = F.current_timestamp().alias(n)            # fill CREATED_AT (table default won’t apply on DF insert)\n        else:\n            expr = provided.get(n, F.lit(None).cast(f.datatype)).alias(n)\n        select_list.append(expr)\n    \n    vals.select(select_list).create_or_replace_temp_view(\"TMP_LOG\")\n    session.sql(f'INSERT INTO {tbl} SELECT * FROM TMP_LOG').collect()\n    \n    # --- normalized buys -> FUEL_RUN_BUYS ---\n    insert_df = session.create_dataframe(plan_preview).select(\n        F.lit(run_id).cast(StringType()).alias(\"RUN_ID\"),\n        F.col(\"EVENT_SEQ\").cast(IntegerType()).alias(\"EVENT_SEQ\"),\n        F.col(\"BUY_GALLONS\").cast(DecimalType(10, 2)).alias(\"BUY_GALLONS\"),\n    )\n    \n    insert_df.create_or_replace_temp_view(\"TMP_BUYS\")\n    session.sql(\"\"\"\n      INSERT INTO DEV_DOT_MLAI.DTI_FUEL.FUEL_RUN_BUYS (RUN_ID, EVENT_SEQ, BUY_GALLONS)\n      SELECT RUN_ID, EVENT_SEQ, BUY_GALLONS FROM TMP_BUYS\n    \"\"\").collect()\n\n    return {\n        \"status\": \"ok\",\n        \"load_number\": inputs.load_number,\n        \"fuel_plan\": plan_preview,\n        \"fuel_plan_text\": fuel_plan_text,\n        \"rows_in_payload\": stops.count(),\n        \"fuel_params\": {\n            \"initial_fuel\": inputs.initial_fuel,\n            \"tank_capacity\": inputs.tank_capacity,\n            \"mpg\": inputs.mpg,\n            \"safety_buffer\": inputs.safety_buffer\n        },\n        \"dc_events_preview\": dc_events_preview,\n        \"state_boundary_preview\": boundary_preview,\n        \"events_unified_preview\": combined_preview\n    }\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c76f359-5853-484b-998e-cc9356a7d469",
   "metadata": {
    "language": "python",
    "name": "validation"
   },
   "outputs": [],
   "source": "def validate_inputs(_inputs):\n    print(_inputs)\n    df = _inputs.stops_df\n    print(\"== schema ==\"); print(df.schema)\n    print(\"== head ==\");  df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15605a18-15aa-4769-af15-80f3c1bc7d78",
   "metadata": {
    "language": "python",
    "name": "launcher_cell"
   },
   "outputs": [],
   "source": "try:\n    _inputs = build_inputs_from_argv(sys.argv, session)\n    validate_inputs(_inputs)\n    result = main(session, _inputs)\n    print(f'\\n\\nRESULTS: \\n\\n{result}\\n\\n')\n\nexcept Exception as e:\n    load_no = getattr(_inputs, \"load_number\", \"UNKNOWN\")\n    summary = f\"Pipeline failed for load {load_no}\\n\" \\\n              f\"Error: {type(e).__name__}: {e}\"\n\n    # Try to notify\n    try:\n        send_error_email(summary, subject=f\"DTI FUEL ERROR: load {load_no}\")\n    finally:\n        raise  # re-raise original error so Snowflake/notebook marks the run as FAILED\n",
   "execution_count": null
  }
 ]
}
